\section{Capitolo 1 - Introduzione}

\subsection{Tipi di collegamento}
Ci sono due tipi di collegamento:
\begin{itemize}
\item \textbf{Poin-to-Point} - la comunicazione avviene tra due macchine tramite dei pacchetti.
\item \textbf{Broadcast} - esiste un canale condiviso da tutte le macchine;
una volta inviato un pacchetto viene ricevuto da tutti e viene processato solo dalla macchina che ha l'indirizzo indicato nel pacchetto;
è possibile anche mandare un pacchetto a tutte le macchine della rete con un codice speciale nel campo indirizzo del pacchetto.
\end{itemize}

\subsection{Classificazione delle reti}
Classificazione delle reti in base alla distanza dei processori:
\begin{itemize}
\item PAN - Personal Area Network
\item LAN - Local (Stanza/Edificio)
\item MAN - Metropolitan (Città)
\item WAN - Wide (Nazione/Continente)
\item Internet (Pianeta)
\end{itemize}

\subsection{Architettura delle reti}
L'architettura delle reti è composta da livelli e protocolli.
Una rete è organizzata come una pila di livelli i quali offrono servizi ai livelli superiori, senza dare dettagli implementativi.

Un livello di un computer comunica con lo stesso livello di un altro computer.
Le entità su uno stesso livello sono detti peer (pari).
Tuttavia i dati non passano direttamente da un livello $n_A$ ad un livello $n_B$, ma dovranno essere passati ai sottolivelli di A per poi risalire i livelli di B.

Le regole e le convenzioni per la comunicazione tra livelli pari sono note come \textbf{protocolli} di livello.
Un protocollo indica come deve avvenire la comunicazione tra le parti (es: il formato e significato dei pacchetti).

Infine, tra i livelli sono presenti le \textbf{interfacce}, che definiscono i \textit{servizi} (cioè un insieme di primitive/operazioni) che il livello inferiore mette a disposizione del soprastante.

In una rete strutturata bene è possibile sostituire l'implementazione di un livello perché quella nuova dovrà solo rendere disponibile al livello soprastante gli stessi servizi della vecchia.

\subsubsection{Progettare una rete}

I punti chiave da tenere in considerazione quando si progetta una rete sono:
\begin{itemize}
\item Affidabilità
\begin{itemize}
\item individuare gli errori e, se possibile, correggerli;
\item trovare un percorso valido tra sorgente e destinatario (routing/instradamento);
\end{itemize}
\item Evoluzione della rete
\item Allocazione delle risorse
\begin{itemize}
\item gestire una banda condivisa;
\item impedire che una sorgente veloce inondi un ricevitore lento (flow control) per evitare una congestione;
\end{itemize}
\item Sicurezza della rete
\end{itemize}

\subsubsection{Tipi di servizi offerti da un livello}
Un livello può offrire un servizio orientato alla connessione o senza connessione.

Il primo richiede di stabilire una connessione, usarla e rilasciarla una volta terminato. 
In questo servizio le informazioni inviate mantengono l'ordine di partenza.

Nel servizio senza connessione ogni pacchetto viene mandato al destinatario indipendentemente dai messaggi successivi, quindi non c'è sicurezza che l'ordine sia mantenuto. 

Inoltre c'è una questione legata all'affidabilità. 
Un servizio è considerato affidabile se riceve sempre tutti i pacchetti e di solito avviene tramite una conferma di ricezione. 
La conferma tuttavia rallenta le prestazioni del servizio, quindi è necessario valutare nei singoli casi se ne vale la pena: 
ad esempio in un servizio di streaming è più importante la velocità per un servizio più fluido piuttosto che l'affidabilità per una qualità migliore dell'audio/video.

Un esempio di primitive di un servizio orientato alla connessione sono: \texttt{Listen, Connect, Accept, Receive, Send, Disconnect}.

\subsection{Modelli di reti}
Esistono due architetture di rete principali utilizzate come modello: OSI e TCP/IP. \\
Il primo utilizza 7 livelli:
\begin{enumerate}
\item fisico
\item data link
\item rete
\item trasporto
\item sessione
\item presentazione
\item applicazione
\end{enumerate}
Il secondo utilizza 4 livelli:
\begin{enumerate}
\item link - livello  di accesso alla rete per spedire i pacchetti IP.
\item internet - livello che gestisce l'indirizzamento dei nodi e l'instradamento, assegnando ad ogni nodo un indirizzo IP e indicando il percorso migliore verso il destinatario.
\item trasporto - livello che gestisce la comunicazione, tramite protocollo TCP o UDP.
\item applicazione - livello più vicino all'utente che gestisce le sessioni e la presentazione; alcuni protocolli di questo livello sono HTTP e DNS.
\end{enumerate}
Il libro e il corso utilizza un modello ibrido a 5 livelli:
\begin{enumerate}
\item fisico - indica come vengono trasmessi i bits tramite segnali elettrici o simili.
\item link - indica come mandare i messaggi ai computer (es. Ethernet, 802.11).
\item rete - indica come combinare link multipli per spedire pacchetti tra computer distanti
\item trasporto - gestisce la comunicazione, tramite protocollo TCP o UDP.
\item applicazione - programmi che usano il network
\end{enumerate}

\newpage
\section{Capitolo 2 - Strato fisico}

L'obiettivo dello strato fisico è quello di trasportare i bits da una macchina ad un'altra.
Per farlo si possono usare diversi mezzi fisici con proprie caratteristiche.
I mezzi di trasmissione possono essere guidati (es: cavi) o non guidati (wireless, satelliti).

Le informazioni possono essere trasmesse su cavi sfruttando proprietà come voltaggio o corrente. 
Usando questi valori, si può modellare il comportamento del segnale e analizzarlo matematicamente.

\subsection{Serie di Fourier e Banda passante}

Fourier afferma che una funzione periodica con periodo T può essere costruita come la somma di un numero n di seni e coseni.
Da ciò deriva la serie di Fourier, una formula che permette di ricostruire la suddetta funzione periodica. 
Nel contesto delle reti, un segnale che ha durata finita può essere immaginato come un pattern che viene ripetuto con l'intervallo T e 2T identico all'intervallo 0 a T.
In questo modo, conoscendo periodo e ampiezza è possibile ricostruire la funzione del segnale, permettendo un analisi e modellazione più facile del segnale.
Il problema è che i mezzi di trasmissione perdono parte della potenza del segnale, generando una distorsione.
Un cavo riesce a trasmettere frequenze senza attenuazione in un intervallo che va da 0 a $f_c$ (frequenza di cutoff).
Questo intervallo è detto Banda passante (\textit{bandwidth}) e dipende da diversi fattori del mezzo di trasmissione (materiali, lunghezza e spessore di un cavo, ecc).
Le frequenze che vanno oltre vengono attenuate.
La frequenza di cutoff non è ben definita, quindi di solito si pone l'intervallo da 0 fino alla frequenza dove la potenza del segnale è dimezzata.
Questi sono detti segnali baseband.
A volte vengono utilizzati dei filtri che possono modificare la banda passante, per esempio alzando l'intervallo da un valore maggiore di zero: è il caso delle trasmissioni wireless.
Questi sono segnali passband.

\subsection{Mezzi di trasmissione guidati}

\subsubsection{Mezzi magnetici}
I mezzi magnetici sono un comune mezzo di trasporto per dati (cd, dcd, hdd) che in alcuni casi può risultare più conveniente se considerato un rapporto dimensioneDati/tempoTrasferimento.
Per esempio può essere più comodo trasportare un camion di hard disk piuttosto che spedire lo stesso quantitativo di dati tramite la rete. 
Anche se le connessioni stanno diventando sempre più veloci, in alcuni casi i mezzi magnetici possono rimanere la miglior soluzione (sempre da considerare il contesto).

\subsubsection{Il doppino}
Il doppino è un cavo composto da due conduttori in rame isolati di 1mm, attorcigliati in modo elicoidale (simile al DNA).
Questa forma permette di eliminare i campi elettromagnetici che si verrebbero a formare se fossero paralleli.
Un segnale è tramesso come differenza di voltaggio tra i due cavi, in modo da evitare i disturbi da rumori esterni(?).
L'uso più comune è per il telefono e l'accesso ad internet con l'ADSL. 
Il doppino può estendersi per chilometri, ma dopo certe distanze è necessario un ripetitore altrimenti il segnale diventa troppo attenuato.
Il prezzo del doppino è ridotto e ha una velocità di trasmissione moderata.
Si possono usare per segnali sia analogici che digitali e la larghezza di banda dipende dallo spessore del cavo e dalla distanza percorsa, tuttavia è limitata.

Esistono diversi tipi di cavi che utilizzano il doppino, come Cat 3 e Cat 5.
Questi consistono in due cavi isolati attorcigliati tra loro, raggruppati con altre coppie (4 totali), ricoperti da una protezione in plastica.
La differenza tra le due tipologie sta nel numero di spire per metro: maggior numero di spire significa una maggior qualità del segnale su lunghe distanze.
Esistono categorie superiori come Cat 6 e 7 che supportano segnali con una maggiore larghezza di banda (fino a 500 MHz).

\subsubsection{Il cavo coassiale}
Il cavo coassiale è un mezzo trasmissivo che permette una maggior larghezza di banda (fino a qualche GHz) rispetto al doppino grazie alla migliore schermatura,
permettendo di viaggiare a maggiori velocità a lunghe distanze.
In particolare, il cavo è composto da un nucleo conduttore in rame, ricoperto da un materiale isolante,
il quale a sua volta è ricoperto da un conduttore cilindrico intrecciato (tipo una rete), protetto da una guaina in plastica.
Esistono due tipi di cavi coassiali, usati in base al tipo di segnale.
Il 50$\Omega$ viene usato per i segnali digitali, mentre il 75$\Omega$ per i segnali analogici e per la televisione.
Questo cavo veniva usato anche nel campo della telefonia, ma ormai sta venendo rimpiazzato dalla fibra ottica. Viene ancora usato per la tv via cavo e per le MAN.

\subsubsection{Fibra ottica}
Un sistema di trasmissione ottico si basa su una fonte luminosa, un mezzo di trasmissione e un ricevitore.
La presenza di luce indica un 1 mentre l'assenza uno 0.
Nel caso della fibra ottica, si utilizza una sottilissima fibra di vetro come nucleo, attraverso la quale viaggia la luce. 
Alle estremità un ricevitore che "legge" i segnali luminosi e li traduce in segnali elettrici.
La buona riuscita della trasmissione del raggio luminoso sta negli indici di rifrazione dei componenti della fibra ottica. 
Grazie ad essi il raggio luminoso rimane nella fibra e continua il suo percorso fino a destinazione.
Infatti il core è ricoperto da un rivestimento di vetro (cladding) che ha un indice di rifrazione più basso, e a sua volta è ricoperto da uno strato protettivo in plastica.
Di solito le fibre sono raggruppati in fasci, a loro volta protetti da una guaina esterna.

In base allo spessore del nucleo, la fibra cambia il proprio nome.
Se un raggio al suo interno è propagato grazie ai rimbalzi della rifrazione, si dice multimodale (50 microns).
Se la fibra è abbastanza sottile da far procedere il raggio quasi in linea retta, si dice monomodale (8-10 microns).
Quest'ultima è più costosa ma più efficiente sulle lunghe distanze.

Ci sono tre modi per connettere la fibra ottica:
\begin{itemize}
\item collegamento della parte finale ad un connettore in apposite prese, con una perdita del 10-20\% del segnale luminoso ma una facile riconfigurazione del sistema
\item attaccate meccanicamente, cercando di allinearle al meglio, con una perdita del 10\% del segnale
\item fusione delle due parti, generando una piccola attenuazione
\end{itemize}

Le fonti luminose possono essere LED (basso data rate, multimodale, low cost) o laser semiconduttori (alto data rate, sia mono che multimodale, costoso).

Il ricevitore che converte il segnale luminoso in elettrico ha un limite di data rate di 100 Gbps.
Inoltre l'interferenza termica può risultare un problema, quindi conviene utilizzare raggi luminosi abbastanza potenti da essere rilevati.

La fibra ottica è una tecnologia relativamente recente, di conseguenza non tutti gli addetti hanno le conoscenze necessarie per installarla od utilizzarla correttamente;
può anche danneggiarsi se si piega troppo. 
Inoltre la trasmissione è monodirezionale, quindi sono richiesti due cavi per andata e ritorno, e le interfacce sono più costose.

Tuttavia ha una maggior ampiezza di banda, richiede meno ripetitori (uno ogni 50km contro uno ogni 5 di quelli in rame), il che porta ad un risparmio,
è più sottile, richiedendo quindi meno spazio, è più sicura perché non è possibile intercettare la luce ed infine è più adatta ai luoghi inospitali, in quanto subisce meno interferenze.

\subsection{Mezzi wireless}

\subsubsection{Spettro elettromagnetico}
Lo spostamento degli elettroni crea onde elettromagnetiche:
il numero di oscillazioni al secondo di un'onda è detta frequenza (misurata in Hz),
mentre la distanza tra due massimi è detta lunghezza d'onda (indicata da lambda).
Un'antenna collegata ad un circuito elettrico riesce a trasmettere onde elettromagnetiche.
Nel vuoto le onde viaggiano tutte alla velocità della luce, nei cavi la velocità scende a 2/3 di quella della luce.

Lo spettro elettromagnetico è composto da diversi tipi di onde in base alla frequenza:
radio, microonde, infrarosso, luce visibile, ultravioletti, raggi x e raggi gamma.
Queste si possono usare per trasmettere segnali; le ultime tre sarebbero le migliori ma non vengono usate per la difficoltà nel generarle e sono dannose per gli esseri viventi.

Di solito si usa una banda di frequenza ristretta per avere una migliore ricezione, ma in alcuni casi si utilizza la banda larga con due varianti:
\begin{itemize}
\item spettro distribuito a frequenza variabile (frequency hopping), dove il trasmettitore salta da una frequenza all'altra centinaia di volte al secondo (adottato dal 802.11)
\item spettro distribuito a sequenza diretta (direct sequence)
\end{itemize}

\subsubsection{Trasmissioni radio}
Le onde radio sono onde a bassa frequenza, facili da generare, che possono viaggiare per lunghe distanze e attraversano facilmente gli edifici. 
Queste onde sono omnidirezionali, cioè si espandono in tutte le direzioni, quindi non necessitano che il trasmettitore e il ricevitore siano allineati.
Le onde radio sono soggette a interferenze da motori e da altri dispositivi elettrici.

A bande basse (VLF, LF, MF), le onde radio seguono il terreno e si possono ricevere fino a 1000km.
Le stazioni radio AM usamo le MF che permettono di attraversare facilmente gli edifici.
Le bande alte (HF, VHF) sfruttano i rimbalzi contro la ionosfera per ottenere trasmissioni a distanze maggiori.

\subsubsection{Trasmissioni a microonde}
Sopra i 100MHz le onde viaggiano quasi in linea retta, rendendo possibile la messa a fuoco.
Si concentra l'energia in un unico raggio trasmesso tramite un'antenna parabolica, tuttavia è richiesto che sia allineata con l'antenna ricevente.
Se da un lato è una limitazione non da poco, dall'altro permette di trasmettere più raggi in parallelo senza interferenze.
Quando le antenne sono lontane, entra in gioco la curvatura della terra e sono quindi necessari dei ripetitori.
Più sono in alto le antenne, maggiore è la distanza raggiungibile.

Esiste un problema con le microonde. Anche se sono dirette, possono divergere e rifrangere sugli strati più bassi dell'atmosfera,
arrivando fuorifase con le dirette, il che può annullare il segnale.
L'effetto è detto multipath fading e può essere determinato dalle condizioni climatiche e dalla frequenza.
La richiesta di spettro ha portato ad utilizzare frequenze più alte, ma queste hanno il problema di venire assorbite dall'acqua. 
In entrambi i casi, la soluzione è interrompere la trasmissione in caso di pioggia e utilizzare altri mezzi.

Nonostante questi problemi, le microonde sono molto utilizzate per le comunicazioni telefoniche a lunga distanza, nella telefonia cellulare e nella televisione.
Rispetto a mezzi come la fibra, basta una semplice antenna e non è richiesto alcun diritto di passaggio. 
\`E inoltre più economica da installare.
Esiste però un altro problema, ovvero il bisogno di più frequenze dello spettro.
Sono stati stipulati degli accordi per gestire le frequenze utilizzabili (Come? Concorso di bellezza, lotteria, non assegnandole). 

\subsubsection{Infrarossi}
Queste onde sono utilizzate per la comunicazione a cortoraggio (esempio i telecomandi).
\`E un sistema economico ma che non attraversa gli ostacoli solidi.
Tuttavia questa limitazione torna comoda in determinate situazioni perché non riuscendo ad attraversare i muri non crea interferenze.

\subsubsection{Trasmissioni a onde luminose}
La trasmissione utilizzando laser è unidirezionale e richiede quindi due laser e due rilevatori.
L'ampiezza di banda offerta è elevata, a costo ridotto e di facile installazione.
Tuttavia puntare il laser richiede molta precisione e non posso attraversare pioggia e nebbia.
Anche le giornate serene possono creare problemi in quanto il caldo può creare correnti di convezione che deviano il raggio.

\subsection{Satelliti}

Un satellite è composto da tanti transponder che ascoltano una diversa porzione dello spettro elettromagnetico.
Quando riceve un segnale in arrivo, il relativo transponder lo amplifica e lo ritrasmette con frequenza diversa per evitare interferenze.
Esistono tre tipi di satelliti in base alla loro posizione: GEO, MEO, LEO.\\
Vai a §\ref{satelliti} per il confronto delle tre tipologie.

\subsection{Rete telefonica pubblica commutata}
Il PSTN, ovvero Public Switched Telephone Network, rete telefonica pubblica commutata, è uno dei sistemi di comunicazione esistenti. 

Il sistema telefonico è strutturato secondo una gerarchia multilivello ad alta ridondanza.
Da ogni telefono partono due cavi di rame collegati alla centrale locale (la più vicina). 
Se la chiamata avviene tra due utenti collegati alla stessa centrale, questa crea una connessione elettrica tra i due e rimane aperta fino al termine della chiamata.
Se invece i due telefoni sono collegati a due centrali diverse, le centrali locali si collegano con una centrale interurbana che crea la connessione. 
Se tuttavia non si collegano alla stessa centrale interurbana, cercano di collegarsi a stazioni intermedie di livello superiore.

I collegamenti locali utilizzano il doppino con segnali analogici, mentre le linee utilizzano le fibre ottiche digitali per collegare le centrali di commutazione.

\subsubsection{Collegamenti locali}
Il collegamento locale, spesso chiamato anche "ultimo miglio", utilizza tutt'ora la trasmissione analogica.
Innanzitutto, per spedire dati digitali devono essere convertiti in analogici dal modem. 
Una volta giunta la centrale, i dati vengono riconvertiti in digitale.
Il ricevente farà la conversione inversa.
Il segnale analogico viene trasmesso tramite la variazione di tensione, quindi il segnale ricevuto non sarà mai identico, il che può determinare errori.
I problemi sono 3:
\begin{itemize}
    \item attenuazione, rappresenta la perdita di energia causata dalla propagazione del segnale e dipende dalla frequenza
    \item distorsione, il segnale si modifica a causa della differenza di velocità con cui si propagano i componenti di Fourier attraverso il cavo
    \item rumore, cioè l'energia indesiderata generata da sorgenti esterne al trasmettitore
\end{itemize}

\paragraph{Modem}
Dato che i problemi descritti sopra sono molto legati alla frequenza, conviene che venga utilizzato un intervallo ridotto.
I segnali digitali tuttavia usano un ampio spettro di frequenza e sono quindi molto soggetti ad attenuazione e distorsione.
Si utilizza come soluzione la trasmissione AC, introducendo un tono continuo, detto portante d'onda sinusoidale, nell'intervallo 1000-2000 Hz.
Modulando ampiezza, frequenza e fase si possono trasmettere le informazioni.
Nella modulazione d'ampiezza di usano due ampiezze diverse per indicare 1 o 0.
Nella modulazione di frequenza si utilizzano due o più toni.
Nella modulazione di fase l'onda portante è spostata di 0 o 180 a intervalli uniformi.

Il modem accetta i bit in ingresso e produce la portante utilizzando i metodi di modulazione (e viceversa).

Il numero di campioni al secondo è misurato in baud. 
Durante ogni baud viene trasmesso un simbolo, quindi una linea a \textit{n} baud trasmette \textit{n} simboli al secondo.
Questa misura è detta Baudrate, che è diverso dal Bitrate.
Il Bitrate indica quanti bit al secondo (bps) vengono trasmessi, mentre il Baudrate indica il numero di simboli trasmessi.
Quindi la differenza tra i due è determinato da quanti bit rappresenta un simbolo ed è determinato dalla tecnica di modulazione utilizzata. 
Se viene utilizzato il voltaggio 0V=0 e 1V=1, allora Bitrate e Baudrate equivalgono, ma se ogni simbolo è composto da 2 o più bit, il Bitrate sarà conseguentemente maggiore.
Quando ci sono 4 possibili cambi di fase e quindi un simbolo descrive 2 bit, la tecnica di modulazione è detta QPSK (Quadrature Phase Shift Keying).

Un'altra tecnica di modulazione è la QAM-16, la quale utilizza 4 bit per simbolo (4 ampiezze e 4 fasi). Se vengono utilizzati 5 bit è detta QAM-32, 6 bit QAM-64.

I diagrammi di costellazione indicano le combinazioni valide di ampiezza e fase. 
Un modem può comunicare solo con altri model che usano la stessa costellazione, ma sono molto soggette ad errori in quanto basta un'alterazione dell'ampiezza o della fase per perdere informazioni.
Per risolvere questo problema sono stati introdotti bit di parità per implementare codice di correzione di errori.
Questi schemi sono detti TCM (Trellis Coded Modulation). Lo standard V32 usa 4 bit di dati e uno di parità.
Il V32 bis usa 6 bit di dati e 1 di parità. Ci sono anche versioni superiori.

I modem moderni permettono la trasmissione bidirezionale utilizzando frequenze diverse.
Una connessione che permette di viaggiare \textbf{contemporaneamente} in entrambi i sensi è detta full duplex, mentre half duplex se solo una direzione è supportata.
Se permette di viaggiare solo in una direzione è detta simplex (es: fibra ottica).

\paragraph{DSL, Digital Subscriber Line}
I collegamenti locali che usano il modem si collegano al commutatore con un filtro che limita la frequenza,
mentre chi utilizza le tecnologie DLS si connettono ad un diverso commutatore che non presenta il filtro.
Quindi il limite non è più il filtro, bensì le proprietà fisiche del mezzo.

Il primo ADSL divideva lo spettro tra:
\begin{itemize}
    \item servizio telefonico
    \item upstream
    \item downstream
\end{itemize}
La tecnica di divisione è detta multiplexing a divisione di frequenza.
Il metodo alternativo è il DMT (Discrete MultiTone), che divide in 256 canali lo spettro.
Il canale 0 è per la voce, i canali 1-5 non usati, uno per upstream e uno per downstream, gli altri a disposizione dei dati dell'utente.
Quelli liberi per i dati di solito vengono distribuiti secondo il provider tra up e down.

\subsection{Linee e multiplexing}
Le aziende telefoniche hanno organizzato il sistema per utilizzare un unico collegamento fisico sia per la banda larga che per quella stretta.
Per fare ciò viene utilizzato il multiplexing, che si distingue in due tipologie di base:
\begin{itemize}
    \item FDM: Frequency division multiplexing, dove lo spettro è diviso in bande di frequenza e ogni utente ha a disposizione solo alcune parti
    \item TDM: Time division multiplexing, dove la banda viene scambiata per intero tra gli utenti per breve tempo
\end{itemize}

\subsubsection{Multiplexing a divisione di frequenza}
Prendiamo un esempio con tre canali con filtri che limitano la banda utilizzabile a 3100Hz;
quando i canali sono uniti in multiplexing, gli viene assegnata più banda per mantenere separati i canali: in questo caso 4000Hz.
Prima dell'unione però vengono aumentate le frequenza in modo diverso, in modo da distinguere i canali e evitare che si sovrappongano quando vengono uniti.

Gli schemi di FDM sono parzialmente standardizzati.
Di solito usa 12 canali a 4000Hz uniti in multiplexing nella banda da 60 a 108 kHz.
Questa unità è detta gruppo. Si possono raggruppare in multiplexing diversi gruppi, creando supergruppi (5 gruppi) o mastergroup (5 supergruppi).

\subsubsection{Multiplexing a divisione di lunghezza d'onda}
Questo multiplexing (WDM) è utilizzato per i canali in fibra ottica, dove viene divisa la lunghezza d'onda.
Più fibre ottiche che trasportano energia a lunghezza d'onda diversa si uniscono in un'unica fibra condivisa nel combinatore ottico;
al termine del cavo condiviso uno splitter divide i segnali in base alla lunghezza d'onda.

Basandosi su un sistema ottico completamente passivo, è più affidabile di FDM.

\subsubsection{Multiplexing a divisione di tempo}
Poiché i cavi in rame sono ancora molto utilizzati, si deve mettere da parte WDM. 
FDM richiede collegamenti elettrici analogici e non può essere controllata da computer.
Invece TDM può essere gestita interamente da dispositivi elettronici digitali ma può essere usata solo per dati digitali.
Di conseguenza la centrale locale deve fare le conversioni analogico-digitale tramite un dispositivo detto codec,
il quale elabora 8000 campioni al secondo, la velocità ottimale secondo il teorema di Nyquist.
Ogni campione è quantizzato in un numero di 8 bit.
Questa tecnica è detta PCM (Pulse Code Modulation). 

Quando viene ricreato il segnale analogico, il risultato può non essere esattamente lo stesso a causa della quantizzazione.
Per ridurre l'errore i livelli di quantizzazione vengono distanziati secondo una scala logaritmica che assegna più bit ai segnali con bassa ampiezza e meno a quelli più ampi.

Nord America e Giappone usano il metodo della portante T1, mentre gli altri paesi usano la portante E1.
T1 ha 24 canali vocali in multiplexing che a turno spediscono 8 bit nel flusso in uscita.

Per la modulazione delta vai a §\ref{deltaMod}.


\subsection{Commutazione (\textit{Switching})}

\subsubsection{Commutazione di circuito}
La commutazione di circuito è la tecnica che crea il percorso fisico tra i due telefoni comunicanti. 
Quando una chiamata arriva ad una centrale di commutazione, viene stabilita una connessione fisica tra la linea entrante e quella d'uscita della centrale.
Questo tipo richiede di configurare il percorso \textbf{prima} di iniziare a trasmettere i dati.

\subsubsection{Commutazione di messaggio}
Questa commutazione non richiede un percorso fisico prestabilito. 
Quando viene inviato un blocco di dati, questo viene inviato alla prima centrale e vengono instradate un passo alla volta. 
Ad ogni passo viene controllato che il blocco ricevuto non contenga errori e poi ritrasmesso.
Una rete che utilizza questa tecnica è detta \textit{store and forward}.
Il blocco può essere di qualsiasi dimensione, richiedendo dischi capaci di memorizzarli temporaneamente, e possono occupare per un tempo considerevole una linea.

\subsubsection{Commutazione di pacchetto}
Per risolvere i problemi della commutazione di messaggio, si utilizza quella di pacchetto che impone una dimensione massima del pacchetto e si assicura che non occupino per troppo tempo una linea.
Un altro vantaggio è che un pacchetto che fa parte di un messaggio può essere mandato prima che finisca di arrivare il successivo.
Come per quella di messaggio, non è richiesta la preparazione fisica del percorso prima della trasmissione.
I pacchetti possono seguire strade diverse e non arrivare in ordine.

La commutazione di circuito riserva l'ampiezza di banda per tutto il percorso, mentre quella di pacchetto no.
Quindi la prima soluzione garantisce il servizio, con spreco di risorse, mentre il secondo no.
Anche quella di pacchetto usa la tecnica \textit{store and forward}. 
Un'altra differenza è che la commutazione di circuito dà libertà di velocità, formato e framing, mentre in quella di pacchetto sono dipendenti dall'onda portante.
Ultima differenza riguarda l'addebito: per il circuito dipende da tempo e distanza, per il pacchetto dipende dal volume dei dati.

\subsection{Sistema telefonico mobile}

\subsubsection{Prima generazione - voce analogica}
Prima versione: sistema telefonico per auto, che usava un pulsante per attivare il trasmettitore e disattivare il ricevitore. Quindi una sola direzione alla volta.\\
Seconda versione: IMTS, usa due frequenze, una per trasmettere e una per ricevere. La frequenza disponibile era limitata, quindi richiedeva del tempo per avere la linea libera.

\textbf{AMPS} è il sistema telefonico mobile avanzato da cui deriva la versione digitale D-AMPS.
Un'area geografica è divisa in celle di 10-20Km, ognuna delle quali utilizza frequenze diverse da quelle vicine.
Il vantaggio di questa organizzazione è che celle vicine (ma non quelle adiacenti) possono usare le stesse frequenze, a differenza del IMTS che si estendeva per 100Km.
Si ottiene quindi una maggiore capacità del sistema e riducendo la grandezza delle celle si riesce ad aumentare ulteriormente la capacità, richiedendo anche meno potenza per i trasmettitori.

Il principale problema è trovare una posizione elevata per le antenne base, che sono gestite dalla stazione base, di solito al centro di ogni cella.
La stazione è composta da un computer e un trasmettitore/ricevitore connesso all'antenna.
Le stazioni sono collegate a dei commutatori per il mobile, cioè il MTSO (Mobile Telephone Switching Office).
Se la rete è piccola si collegano allo stesso commutatore, altrimenti si crea una gerarchia a livelli simile alla rete telefonica cablata.
Viene utilizzata la commutazione di pacchetto.

Quando un telefono abbandona una cella perché il segnale di sta affievolendo, la stazione base verifica la potenza del segnale delle celle adiacenti e trasferisce la gestione del dispositivo.
Il telefono è informato del cambiamento e se era in corso una chiamata viene forzato a passare su un altro canale. 
Questo processo è detto \textbf{handoff}. Esistono due tipi:
\begin{itemize}
    \item \textbf{soft handoff}, dove la nuova stazione acquisisce la gestione del telefono prima di interrompere il segnale;
    non c'è perdita di continuità ma il telefono deve essere capace di gestire le due frequenze contemporaneamente (solo la terza generazione di telefoni riesce)
    \item \textbf{hard handoff}, la vecchia stazione rilascia il telefono prima che la nuova lo acquisisca;
    è un processo abbastanza veloce, ma la chiamata può venire interrotta se la stazione non è in grado di gestire il nuovo dispositivo
\end{itemize}
AMPS usa 832 canali duplex composti da una coppia di simplex. 
AMPS usa FDM (Frequency Division Multiplexing) per separare i canali.
I canali sono divisi in 4 categorie:
\begin{itemize}
    \item controllo, dalla base al telefono, per gestire il sistema
    \item paging, dalla base al telefono, per notificare la chiamata all'utente
    \item accesso, bidirezionale, per impostare chiamata e canale
    \item dati, bidirezionale, per voce e dati
\end{itemize}
Ogni telefono ha una PROM dove sono predeterminati 21 canali per il controllo, il numero seriale e il numero di telefono. 
Il telefono trasmette queste informazioni in un pacchetto in broadcast per collegarsi alla stazione base più vicina.

Quando si effettua una chiamata, viene trasmesso tramite il canale di accesso il numero da chiamare e la propria identità.
Una volta che la richiesta raggiunge la stazione, questa comunica con il MTSO che cerca un canale libero per la chiamata.
Quando trovato, viene trasmesso il numero nel canale di controllo e il telefono passa al canale vocale in attesa di risposta.

\subsubsection{Seconda generazione - voce digitale}
Per la seconda generazione si passa al digitale e sono utilizzati 4 sistemi qui esposti.

\paragraph{D-AMPS}
\`E la seconda generazione di AMPS, orientata al digitale, e viene usata principalmente negli Stati Uniti e in Giappone.
La versione digitale utilizza le stesse frequenze di AMPS, quindi vengono divisi i canali tra digitali e analogici, permettendo la compatibilità di dispositivi di entrambe le generazioni.

La ripartizione può cambiare dinamicamente in base al tipo di dispositivi presenti nella cella. 
Sta al MTSO della cella la gestione della distribuzione.

Per gestire l'aumento di carico, è stata prevista una nuova banda di frequenza. 

Il segnale vocale viene raccolto dal microfono, digitalizzato e compresso dal \textit{vocoder}.
Questo processo è molto importante per la telefonia mobile con D-AMPS perché offre un ottimo miglioramento utilizzando il TDM,
tramite il quale tre utenti possono condividere una singola coppia di frequenze. 

Una differenza tra AMPS e D-AMPS sta nella gestione dell'handoff: 
dato che un cellulare per un terzo del tempo non trasmette perché è il turno degli altri dispositivi in multiplexing, 
ha il tempo di valutare la qualità del servizio e se sta degradando avvisa il MTSO che cambia la stazione di base.
Questa tecnica è detta MAHO, ovvero Mobile Assisted HandOff.

\paragraph{GSM}
GSM (Global System for Mobile communications) viene utilizzato dal resto del mondo ed è in parte simile a D-AMPS:
sistemi cellulari, che usano il FDM con trasmissione su una frequenza e ricezione su una più alta;
inoltre per ogni coppia di frequenze è utilizzato il TDM.
I canali di GSM però sono più ampi e contengono 8 utenti, quindi risulta più veloce di D-AMPS e con una maggior qualità della voce.\\
GSM ha un multiframe con diversi canali di controllo. Ad esempio:
\begin{itemize}
\item canale di controllo broadcast è un flusso di dati che annuncia identità e stato del canale della stazione base
\item canale di controllo dedicato aggiorna la posizione, registrare il terminale nella rete e configurare la chiamata
\item canale di controllo comune, diviso in 3 sottocanali:
\begin{itemize}
\item canale di paging annuncia le chiamate in arrivo
\item canale ad accesso casuale permette agli utenti di ottenere uno slot sul canale di controllo dedicato
\item canale di assegnazione dell'accesso annuncia lo slot assegnato
\end{itemize}
\end{itemize}

\paragraph{CDMA}
CDMA (Code Division Multiple Access) è completamente diverso dai precedenti: 
non punta a dividere l'intervallo di frequenze, bensì permette ad una stazione di ottenere tutto lo spettro per quanto tempo vuole.
Le trasmissioni multiple sono separate secondo la teoria della codifica.

Ogni tempo bit è suddiviso in \textit{m} intervalli detti chip (di solito sono 64 o 128 chip). 
Ogni stazione ha un codice m-bit univoco detto sequenza di chip;
per inviare un 1 viene spedita la sequenza, mentre per uno 0 viene mandato il complemento a 1 della propria sequenza.

Durante ogni tempo bit, una stazione può inviare un 1, uno 0, o rimanere in silenzio.
Si ipotizza che i chip delle stazioni siano sincronizzati.
Se più stazioni trasmettono contemporaneamente, i segnali si sommano e si ottiene una sequenza di chip derivata dalla somma delle sequenze.
Il ricevitore calcola il prodotto interno normalizzato della sequenza ricevuta per ottenere i dati inviati, distinti dagli altri.
Per farlo deve conoscere la sequenza di chip del trasmettitore.




Idealmente la capacità di un sistema CDMA potrebbe essere grande, in pratica i limiti fisici diminuiscono la capacità possibile.
Innanzitutto è impossibile che tutti i chip siano sincronizzati.
Inoltre il ricevitore dovrebbe conoscere il trasmettitore, ma non è così semplice. 

CDMA opera su una banda di 1.25 MHz (più di metodi precedenti), ma riesce ad ospitare molti più utente con un'ampiezza di banda per utente superiore a GSM.

\paragraph{PDC}
Usato solo in Giappone, non approfondito.

\subsubsection{Terza generazione - voce e dati digitali}
Per la terza generazione si puntava ad un'unica tecnologia per semplificare la diffusione e lo sviluppo dei dispositivi che dovevano utilizzarla.
Ci sono state diverse proposte e dopo una selezione rimasero due possibilità: W-CDMA e CDMA2000.
Il primo utilizzava una modulazione a spettro distribuito a sequenza diretta, pensato per interagire con GSM, in modo da poter entrare nelle celle di quest'ultimo senza perdere le chiamate. L'Unione Europea spinse su questo e lo chiamò UMTS.
La seconda proposta si basava anch'essa sulla modulazione a spettro distribuito a sequenza diretta, ma non punta all'interazione con GSM.
Inoltre ha altre differenze con W-CDMA, come la frequenza di frammento, tempo di frame, spettro e sincronizzazioni diversi.
Entrambi i metodi usavano una banda di 5MHz.

Nel frattempo alcuni operatori proposero alcuni schemi detti \textit{2.5G}; 
uno è EDGE, un GSM con un bit in più per baud. Il bit in più porta anche a più errori, per cui viene dedicata più banda per la correzione d'errori.

L'altra proposta è GPRS, una rete a pacchetti costruita sopra D-AMPS e GSM.
Questa permette di inviare/ricevere pacchetti IP in una cella vocale;
vengono riservati alcuni slot temporali al traffico di pacchetti e la stazione base definisce il numero e la posizione degli slot.
Gli slot disponibili sono divisi in canali logici, la stazione base determina l'associazione tra i canali logici e time slot.
Un canale logico è usato per scaricare i pacchetti dalla stazione base nella stazione mobile e ogni pacchetto indica il destinatario.

Per inviare un pacchetto IP, una stazione mobile chiede uno o più slot inviando una richiesta alla stazione base.
Se la richiesta arriva senza problemi, la stazione comunica all'apparecchio mobile la frequenza e gli slot che dovrà utilizzare per trasmettere il pacchetto.
Una volta arrivato alla stazione base, il pacchetto è trasferito su Internet attraverso una connessione via cavo.

\newpage
\section{Capitolo 3 - Strato data link}

\subsection{Progetto dello strato data link} % 3.1
Lo strato data link ha diverse funzioni, tra cui:
\begin{itemize}
    \item fornire un servizio di interfaccia per lo strato network
    \item gestire gli errori di trasmissione
    \item regolare il flusso dati 
\end{itemize}
Per svolgere le sue funzioni, lo strato data link riceve i pacchetti dallo strato network e li incapsula in frame.
Un frame contiene header, carico e coda.

\subsubsection{Servizi forniti allo strato network}
Il servizio principale è quello di trasferire i dati tra gli strati network di due macchine differenti.
Alcuni servizi forniti sono:
\begin{itemize}
    \item unacknowledged senza connessione
    \item acknowledged senza connessione
    \item acknowledged orientato alla connessione
\end{itemize}
Il primo consiste nell'invio di frame senza che la macchina di destinazione debba dare l'acknowledgement, ovvero la conferma della ricezione.
Non viene creata una connessione logica e non ci si assicura che il frame arrivi a destinazione.
Si utilizza quando gli errori di trasmissione sono limitati, così da permettere la correzione direttamente agli strati superiori e
quando si vuole una comunicazione real-time dove il ritardo è peggio di una ricezione di dati parzialmente errati (es: chiamate, stream video).

Il secondo offre una maggiore affidabilità in quanto è richiesto l'acknowledgement.
Se non viene ricevuto entro un certo intervallo, può essere rispedito il frame. 
Anche questo non usa una connessione logica. 
L'acknowledgement può essere richiesto direttamente dallo strato network, ma si presenta un problema di prestazioni.
Il strato network può richiedere la spedizione dell'intero pacchetto nel caso non arrivi l'acknowledgement, che non ha limiti di dimensioni.
Un frame invece è di dimensione ridotta e limitata, per cui è più facile gestire il reinvio del singolo frame rispetto al pacchetto intero.

Il terzo tipo è il più affidabile. Oltre alla richiesta di acknowledgement, deve stabilire una connessione con il destinatario prima di iniziare la trasmissione.
I frame sono numerati ed è garantito l'arrivo nell'ordine corretto.
Il trasferimento dei dati avviene in 3 fasi: stabilita la connessione, trasmissione dei frame, rilascio della connessione. 

\subsubsection{Suddivisione in frame}
Lo strato data link deve ricevere dallo strato fisico i dati sottoforma di bit e verificare che siano corretti ed eventualmente correggerli.
Per fare ciò viene suddiviso il flusso di bit in una serie di frame e viene calcolato il checksum (vai a §\ref{checksum}) per ogni frame.
Una volta giunto a destinazione il frame, viene ricalcolato il checksum e se è differente da quello del frame, lo strato sa che c'è stato un errore e prende i relativi provvedimenti.

La suddivisione non è un'operazione banale. Ci sono diversi metodi per farlo, spiegati di seguito.

\paragraph{Conteggio dei caratteri}
Questo metodo usa un campo dell'intestazione per indicare il numero di caratteri nel frame.
In questo modo quando viene letto il frame sa dove termina.
Il problema è che un errore può alterare il conteggio, mandando il destinatario fuori sincronia.
Anche utilizzando il checksum non è possibile sapere dove inizia il frame successivo.
Per questi problemi, il metodo non è più usato.

\paragraph{Byte stuffing}
Un secondo metodo utilizza un byte all'inizio e alla fine del frame come flag byte.
Di solito i byte utilizzati sono gli stessi.
Questo metodo permette di trovare il frame successivo nel caso un errore faccia perdere la sincronia.
Tuttavia il problema di questo metodo si presenta quando vengono trasmessi file binari, in quanto il valore del flag byte potrebbe essere presente nel frame.
Per risolvere questo problema è possibile usare la tecnica del Byte stuffing,
ovvero viene inserito un carattere di escape prima di ogni occorrenza nel frame del byte flag, in modo da distinguerli effettivamente dai flag;
lo strato data link di destinazione rimuoverà gli escape.

\paragraph{Bit stuffing}
Il Byte stuffing è limitato perché è legato a caratteri di 8 bit, ma se si usano codifiche diverse risulta un problema.

Con la nuova tecnica è possibile usare sia un numero arbitrario di bit, sia codifiche varie.
Ogni frame comincia e finisce con il gruppo di bit 01111110, di fatto un flag byte. 
Se nel flusso vengono letti 5 bit 1 consecutivi, viene messo un bit 0 in coda, mentre nella destinazione verrà rimosso.
Questa tecnica è detta bit stuffing.

\subsubsection{Controllo errori}
Problema: assicurarsi che i frame arrivino tutti a destinazione e nell'ordine corretto.\\
Un modo è quello di utilizzare un acknowledgement, positivo o negativo, per notificare l'arrivo del frame.
Tuttavia, anche l'acknowledgement può andare perso. Si utilizza quindi un timer che parte insieme all'invio del frame.
Se viene ricevuto l'acknowledgement, viene ignorato il timer, altrimenti allo scadere del tempo viene rispedito il frame.
Per evitare doppioni, i frame hanno un numero di sequenza in modo che se la destinazione aveva già ricevuto il frame possa ignorarlo.

\subsubsection{Controllo di flusso}
Il controllo del flusso consiste nella gestione della velocità di trasmissione dei frame,
in modo da non congestionare il destinatario se non riesce ad elaborare ciò che riceve abbastanza velocemente.

\subsection{Rilevazione e correzione errori} % 3.2
Gli errori di trasmissione è un problema difficile da evitare, in particolare per i collegamenti locali che sono ancora analogici.

\subsubsection{Codici per la correzione degli errori}
Per gestire gli errori ci sono due possibilità. 
O si aggiungono dei dati ridondanti di parità che permettono di ricostruire il contenuto del blocco (codifica a correzione d'errore),
o si utilizza abbastanza ridondanza da poter identificare la presenza di un errore, senza però poterla correggere, e vengono quindi ritrasmessi (codifica a rilevazione d'errore).
I canali affidabili come la fibra utilizzano la rilevazione, mentre quelli più soggetti a disturbi come il wireless usano la correzione.

Ma in cosa consiste un errore? \\
Un frame è composta da m bit di dati e r bit ridondanti, con un totale di n bit.
Gli n bit sono detti codeword. 
Date le due parole, si possono confrontare con lo XOR per vedere quanti bit sono differenti;
il numero di bit è detta la distanza di Hamming ed indica il numero di errori su singoli bit per convertire da una sequenza all'altra.

La rilevazione ela correzione degli errori dipende dalla distanza di Hamming.
Per trovare d errori è necessaria una codifica con d+1 di distanza;
in questo modo riesce a rilevarli, ma non a correggerli.
Per corregere d errori è invece necessaria una codifica con 2d+1 di distanza,
in modo che anche con d cambiamenti, la codeword originale rimane la più vicina a quella modificata ed è ricavabile univocamente.

Le codifiche di Hamming riescono a correggere solo errori singoli.
Si riesce a corregere gli errori burst sfruttando un trucco, ovvero inviando i dati di una matrice per colonna invece che per riga.

\subsubsection{Codifiche a rilevazione d'errore}
Su canali più affidabili, la rilevazione basta in quanto gli errori sono meno frequenti e risulta più efficiente rimandare i dati danneggiati.
Questo perché maggiore è la grandezza del blocco, maggiore è il numero di bit di controllo richiesti.

In alternativa alla tecnica della matrice, si usa la codifica polinomiale (CRC - Cycle Redundancy Check).
Questa tecnica considera le sequenze di bit come polinomi a coeficienti con valori uguali a 0 o 1.
Un frame di k bit è un polinomio di grado k-1 con k termini.\\
Esempio: 110001 -> $1*x^5+1*x^4+0*x^3+0*x^2+0*x^1+1*x^0$ -> $x^5+x^4+1$\\

Per utilizzare la codifica polinomiale, sorgente e destinazione devono definire un polinomio generatore G(x).
I bit di ordine più alto e più basso devono essere uguali a 1.
Il checksum di un frame è calcolabile se il polinomio relativo al frame è di grado maggiore (banalemente deve avere più bit) rispetto al polinomio generatore.
Si aggiunge un checksum alla fine del frame in modo che il relativo polinomio sia divisibile per G(x).
Se c'è un resto, significa che c'è stato un errore.

Calcolare in checksum:
\begin{enumerate}
    \item dato il grado g di G(x), aggiungere g bit con valore zero in coda al frame, ottenendo il polinomio M(x)
    \item fare la divisione M(x) / G(x)
    \item sottrarre il resto dalla M(x); ne risulta il frame con il checksum pronto alla trasmissione con polinomio T(x)
\end{enumerate}
Divisione e sottrazione vanno effettuate in modulo 2: la sottrazione equivale allo XOR, mentre la divisione sono fatte come nel binario normale ma con le differenze in modulo 2.

Questo metodo riesce a risolvere gli errori provocati da interferenza, rumore e distorsione.

\subsection{Protocolli data link elementari} % 3.3
Un frame è composto da 4 campi: kind, seq, ack, info.
I primi 3 contengono informazioni di controllo e fanno parte dell'header, mentre info contiene i dati effettivi.
Kind indica se ci sono dati nel frame. Seq contiene i numeri di sequenza. Ack contiene l'acknowledgement.

Lo strato network passa un pacchetto con un'intestazione di tipo network allo strato data link, il quale pone il pacchetto nel campo info di un frame.

\subsubsection{Simplex senza restrizioni}
Protocollo in cui i dati sono trasferiti in una sola direzione (simplex) e non ci sono altre restrizioni
(buffer infinito, strati pronti, tempo di elaborazione ignorabile, in canale non perde mai frame).
E' chiaramente un protocollo non realistico, solo d'esempio.

Il mittente invia i dati alla massima velocità possibile in un loop infinito.
Nel loop viene preso il pacchetto dallo strato network, costruisce il frame e instrada il frame. 
Il destinatario attende l'arrivo di un frame, salvato nel buffer;
viene prelevato dal buffer e passato il contenuto del frame allo strato network, infine torna ad attendere.

\subsubsection{Simplex stop-and-wait}
In questo protocollo si assume il traffico simplex e l'assenza di errori, ma con la restrizione più realistica di una velocità di elaborazione non più infinita.
Il problema da considerare quindi è la gestione del flusso, in modo che il mittente non inondi il ricevente con una velocità di trasmissione maggiore di quella di elaborazione.
Se ci vuole un intervallo t per elaborare un frame, il mittente non dovrebbe trasmettere più di un frame per t.
Se inoltre non ci sono meccanismi di buffer, il mittente non deve trasmettere il nuovo frame finché il precedente non è stato prelevato dallo strato fisico o verrebbe sovrascritto.
La soluzione al problema è l'utilizzo di un frame senza informazioni che viene rispedito al mittente quando il pacchetto è stato fatto passare allo strato network,
in modo da notificare il mittente per l'invio di un nuovo frame.
Il pacchetto funge da acknowledgement e blocca il mittente fino a quando non arriva il frame.
Questi protocolli basati sull'attesa dell'acknowledgement sono detti \textbf{stop-and-wait}.
Data la richiesta di far viaggiare i frame in entrambe le direzioni, è necessaria un'alternanza del flusso e un canale fisico half-duplex può bastare.

Il mittente prende il pacchetto dallo strato network, costruisce il frame e instrada il frame. 
A questo punto attende l'acknowledgement; quando lo riceve, manda un altro frame.
Il destinatario invece processa il frame e prima di tornare in attesa manda il frame di acknowledgement al mittente.

\subsubsection{Simplex per canali rumorosi}
Protocollo per una situazione normale, sempre simplex, dove i frame possono essere danneggiati o persi.
Se il frame è danneggiato, viene usato il checksum per verificare la correttezza.
Per far pronte al problema della perdita di frame, sia dati, sia di acknowledgement, si utilizza un timer e il numero di sequenza dei frame.
Il problema successivo è decidere quanti bit usare per il numero di sequenza.
L'unica ambiguità possibile è tra il frame stesso e il successivo, quindi è sufficiente un bit.

Il mittente prima memorizza in una variabile locale il numero di sequenza del successivo frame da inviare, poi crea il frame, lo instrada e fa partire il timer.
Rimane in attesa: riceve un acknowledgement positivo, o un acknowledgement con problemi, o il scade il timer.
Nel primo caso prende in carico il frame successivo, negli altri due viene rimandato il frame.
Il destinatario riceve il frame, controlla il numero di sequenza: 
se è valido, lo processa, lo passa allo strato network, ritorna l'acknowledgement e aggiorna la variabile del numero di sequenza.

\subsection{Protocolli sliding window} % 3.4
Questi protocolli sono utilizzati per le trasmissioni in entrambe le direzioni, full duplex.
Si possono usare due canali separati simplex, ma di solito quello di ritorno è sprecato; conviene usare un unico canale e gestire entrambe le direzioni.
Una miglioria successiva è la tecnica del piggy-backing, ovvero quando arriva un frame dati, non viene restituito subito il frame di acknowledgement, 
bensì viene aggiunto al frame di dati successivo passato dallo strato network, in modo da "risparmiare" un viaggio e sfruttare al meglio la banda.
Infatti aggiungere il campo ack nell'intestazione è più conveniente rispetto a creare un intero frame con sua intestazione, checksum e acknowledgement.
Il problema di questa tecnica è l'estendersi dell'attesa dell'acknowledgement.
Per quanto tempo deve aspettare lo strato data link prima di mandare l'acknowledgement in maniera indipendente?

I protocolli seguenti sono strettamente legati al buffer, in quanto il mittente avrà una "finestra d'invio", ovvero un insieme di frame che può inviare,
e il destinatario ha una "finestra di ricezione", ovvero un insieme di frame che può ricevere.
Le due finistre non devono essere grandi uguali e possono avere dimensione variabili.
Queste danno una maggior libertà di invio dei frame, ma il passaggio allo strato network deve comunque mantenere l'ordine.
I numeri di sequenza nella finestra d'invio indica i frame trasmessi o in transito, che sono attesa dell'acknowledgement.
Quando arriva un pacchetto dallo strato network il limite superiore aumenta di uno, mentre quando arriva un acknowledgement il limite inferiore aumenta di uno.
Se la finestra di invio contiene n frame, il mittente dovrà avere almeno n buffer per mantenere i frame fino all'arrivo dell'acknowledgement.

\subsubsection{Sliding window a 1 bit}
Questo protocollo utilizza una finestra di un bit, risultando quindi simile al protocollo stop-and-wait.
La differenza è che ora la trasmissione è full duplex.
Di conseguenza può accadere che entrambi i computer vogliano mandare un pacchetto,
creando ridondanza in quanto vengono mandati pacchetti duplicati senza che siano necessari.

\subsubsection{Go back N e Ripetizione selettiva}
Il tempo per trasmettere un frame sommato al tempo per ricevere l'acknowledgement a volte non è trascurabile (nei precedenti protocolli era considerato tale).
Infatti si può arrivare a sprecare fino al 90\% di tempo durante il quale il sorgente rimane in attesa per inviare un nuovo pacchetto.
Il problema deriva dalla richiesta di far attendere il sorgente.
Con la tecnica detta pipelining si attenua il problema, 
inviando n frame nell'intervallo totale di transito (dall'invio alla ricezione dell'ack) senza riempire completamente la finestra.
Il numero di frame inviabili indica la grandezza della finestra.
Maggiore è il ritardo di trasmissione, maggiore sarà la grandezza della finestra.
Il prodotto tra banda e ritardo di trasmissione indica la capacità della pipeline.
Usandola tutta, il sorgente opera a massima efficienza.
Dati: 
\begin{itemize}
    \item c - capacità del canale
    \item d - dimensione del frame
    \item T - tempo totale di propagazione
\end{itemize}
l'utilizzo della linea è uguale a d/(d+cT).
Se d<cT, l'efficienza sarà minore del 50\%.

Con il pipelining, se dei frame vengono danneggiati o persi si verificano diversi problemi.
Il destinatario non sa cosa fare con i frame seguenti a quello danneggiato.
Per ripristinare gli errori ci sono due tecniche, una delle quali è il go back N.

Questa tecnica richiede di scartare tutti i frame successivi a quello danneggiato senza mandare l'acknowledgement per questi.
In particolare il sorgente manda tutto in sequenza, non sapendo se arrivano errati a destinazione, finché non termina i frame della finestra.
Quando scade il timer dell'acknowledgement che non sarà ricevuto, il mittente rimanda i frame in sequenza a partire da quello danneggiato,
anche se i successivi erano corretti.
Il destinatario invece, quando riceve il frame danneggiato si rifiuta di accettare tutti i successivi finché non riceve quello con il numero di sequenza atteso.
Di fatto questo protocollo utilizza una finestra di ricezione con 1 frame.

L'altra tecnica per ripristinare gli errori è la ripetizione selettiva.
In questo caso viene scartato solo il frame danneggiato, mentre i successivi corretti vengono messi in un buffer.
Quando il sorgente va in timeout rimanda il frame danneggiato.
Il destinatario lo riceve e passa allo strato network il frame ricevuto più tutti quelli nel buffer, mantenendo l'ordine corretto e ritornando l'ack.
In particolare, quando il ricevente trova un frame danneggiato, viene mandato un NAK, un acknowledgement negativo.
Il NAK migliora le prestazioni in quanto triggera il sorgente a rimandare il pacchetto prima che scada il timer.
Se venisse perso il NAK, ci sarebbe comunque il timer del sorgente.
Questo protocollo utilizza una finestra di ricezione maggiore di 1.
La dimensione massima della finestra dovrebbe essere uguale alla metà della sequenza dei numeri;
invece il numero di buffer della destinazione deve essere uguale alla dimensione della finestra.

I due approcci dipendono molto dall'uso della banda e del buffer. 
Il primo si basa sulla banda, dovendo rimandare tutto, il secondo sul buffer, dato che salva i frame corretti in attesa di quello mancante.

\subsection{Esempi di protocolli} % 3.6

\subsubsection{HDLC - High-level Data Link Control}
Questo protocollo è orientato ai bit e utilizza il bit stuffing.
Il frame di questo protocollo ha il campo address, control, data e checksum.
Address indica il terminale; control è usato per i numeri di sequenza; 
data contiene le informazioni; checksum è il codice di ridondanza.
Il frame è delimitato dal flag 01111110.
[...]

\subsubsection{PPP - protocollo punto a punto}
Il PPP è un protocollo data link usato da internet.
Tra le caratteristiche ci principali ci sono:
\begin{itemize}
    \item metodo di framing
    \item protocollo per la gestione della connessione - LCP (Link Control Protocol)
    \item una modalità per negoziare le opzioni dello strato network in modo indipendente dalla sua implementazione
\end{itemize}

Il frame PPP assomiglia a quello HDLC, tuttavia è orientato ai caratteri, quindi usa il byte stuffing.
Il flag usato è sempre quello di HDLC; i caratteri di escape vengono aggiunti su tutto il frame (non solo sui dati)
dopo il calcolo del checksum e vengono tolti in ricezione prima del ricalcolo del checksum.
Ha il campo address sempre impostato a 11111111 per indicare che tutte le stazioni devono accettarlo;
segue il campo control che di default vale 00000011.
Dato che questi campi sono costanti, LCP permette di omettere questi campi nella configurazione.
Il campo protocol indica il tipo di pacchetto contenuto nel campo payload.
L'ultimo campo contiene il checksum.
[...]

\newpage
\section{Capitolo 4 - Sottostrato MAC}
Il sottostrato MAC riguarda i canali multiaccesso, ovvero i canali usati delle reti broadcast.
C'è bisogno di regolamentare l'accesso al canale in qualche modo.

\subsection{Il problema dell'assegnazione del canale} % 4.1
Come assegnare un singolo canale broadcast tra diversi utenti in competizione?

\subsubsection{Assegnazione statica}
Di solito per condividere un canale tra più utenti si usa il multiplexing a divisione di frequenza FDM.
La banda viene suddivisa in parti uguali le quali sono assegnate agli utenti. 
Ognuno ha una proprio frequenza, quindi non c'è interferenza tra loro.
FDM funziona bene se non ci sono troppi utenti e se il numero è abbastanza costante.
Se non lo è, ci sono dei problemi. 
Se gli utenti sono minori della divisione della banda, ci sarà uno spreco;
se sono di più, alcuni utenti non riusciranno a comunicare.

In generale, un'assegnazione statica è poco efficiente perché anche mantenendo costanti gli utenti,
quando non usano il canale quella banda rimane inutilizzata.
Anche con TDM la situazione non cambia molto.

\subsubsection{Assegnazione dinamica}
Ci sono 5 premesse riguardo il problema dell'assegnazione del canale.
\begin{itemize}
    \item Modello della stazione: n stazioni indipendenti che generano frame in trasmissione;
    quando viene generato un frame, la stazione attende finché non viene spedito
    \item Presupposto del canale singolo: un solo canale per trasmettere e ricevere
    \item Presupposto della collisione: due frame trasmessi assieme di sovrappongono creando distorsione e rendendo il frame non valido;
    le stazioni possono rilevare questo evento detto collisione e richiedere la ritrasmissione del frame
    \item Tempo continuo e Tempo diviso in intervalli: nel primo caso la trasmissione può avvenire in qualsiasi istante,
    mentre nel secondo il tempo è diviso in slot e la trasmissione di un frame coincide sempre con l'inizio di un intervallo
    \item Occupazione del canale verificabile o meno: le stazioni possono o meno verificare se un canale è occupato;
    se possono verificare, la trasmissione dipende dall'esito, se non possono mandano il frame a priori
\end{itemize}

\subsection{Protocolli ad accesso multiplo} % 4.2

\subsubsection{Aloha}
Aloha è un sistema sviluppato inizialmente per collegare le isole della Hawaii, basato su una trasmissione radio broadcast con singolo canale.
Ne esistono due versioni.

\paragraph{Aloha puro}
L'idea di base è quello di consentire agli utenti di trasmettere ogni qual volta ne avessero bisogno.
Anche se ci sono collisioni, la proprietà di feedback della trasmissione broadcast permette al trasmettitore di scoprire se il frame è danneggiato.
Se non è possibile ascoltare il canale, è necessario un sistema di acknowledgement.
Se il frame è stato distrutto, il trasmettitore attenderà per un tempo \textbf{casuale} prima di ritrasmettere il frame altrimenti si ripeterà la collisione.
I sistemi con un canale condiviso dove si possono generare conflitti si dicono sistemi a contesa.

I frame trasmessi sono tutti della stessa lunghezza e vengono inviati completamente a caso. 
Se anche un solo bit va in collisione, entrambi i due frame vanno buttati.
Non c'è modo di capire se c'è stato un conflitto parziale o totale.

\paragraph{Aloha slotted}
Alcuni anni dopo venne proposta l'alternativa slotted Aloha,
la quale permette di duplicare la capacità del sistema dividendo il tempo in intervalli discreti, dove ogni intervallo corrisponde un frame.
In questo sistema non viene inviato un frame in un momento a caso, ma solo all'inizio dell'intervallo successivo.
Questa soluzione riduce le collisioni, ma richiede un meccanismo di sincronizzazione per gestire l'invio dei frame solo all'inizio di un intervallo.

Questa tecnica fu rispolverata quando fu inventato l'accesso a internet via cavo, dovendo gestire un canale condiviso.

\subsubsection{Protocolli ad accesso multiplo con rilevamento della portante}
Con i protocolli Aloha si hanno molte collisioni.
Se invece le stazioni rimangono in ascolto di una portante è possibile migliorare le prestazioni.

\paragraph{CSMA persistente e non}
Un esempio di questi protocolli è il CSMA 1-persistente.
Una stazione prima di trasmettere ascolta il canale per sapere se è occupato.
Se lo è attende e appena si libera manda un frame;
in caso di collisione la stazione rimane in attesa di un intervallo casuale per poi rimandare il frame.
1-persistente indica che la stazione trasmette con probabilità 1 quando il canale è libero (cioè lo fa sempre).
Il problema è che il ritardo di propagazione potrebbe creare l'illusione di avere un canale libero,
quando in verità il canale è già stato occupato ma il segnale non ha ancora raggiunto la stazione che ne richiede l'utilizzo.
Anche con ritardo zero potrebbe verificarsi una collisione in quanto due stazioni potrebbero attendere contemporaneamente.
Questo protocollo risulta migliore dell'Aloha puro.

Un altro protocollo è detto CSMA non persistente, il quale se trova il canale occupato non rimane subito in attesa, bensì riprova dopo un intervallo casuale.
Si ha un migliore utilizzo del canale ma aumenta i ritardi.

Esiste anche il CSMA p-persistente, dove p indica la probabilità di trasmettere nel canale se è libero.
A differenza del 1-persistente, la probabilità sarà minore di 1, quindi potrebbe non trasmettere subito e rimandare a più tardi.

\paragraph{CSMA con rilevamento delle collisioni}
Esiste un miglioramento dei CSMA persistenti, con un protocollo detto CSMA/CD (collision detection).
In questo protocollo, quando una stazione inizia la trasmissione e rileva la collisione, termina subito la trasmissione;
dopo un intervallo di tempo casuale ritenta la trasmissione.
In questo modo si risparmia tempo e banda. Viene molto usato nel sottostrato MAC delle LAN.

Piccola nota: nessun protocollo del sottostrato MAC garantisce consegna affidabile.
Potrebbero non esserci collisioni, ma il ricevitore potrebbe comunque copiare in frame non correttamente.

\subsubsection{Protocolli senza collisione}
Nei seguenti protocolli si presuppone che ci siano N stazioni identificate da un indirizzo che va da 0 a N-1 e che il ritardo sia trascurabile.
Questi protocolli risolvono la contesa senza creare collisioni.

\paragraph{Protocollo a mappa di bit elementare}
In questo protocollo, ogni periodo di contesa è composto da N intervalli.
Ogni stazione indicherà tramite un bit se deve trasmettere un frame. 
Il bit viene posto nel rispettivo intervallo in base all'indirizzo della stazione (stazione 0 in 0, stazione N-1 in N-1).
Al termine degli N intervalli, tutte le stazioni sono a conoscenza di chi ha bisogno di trasmettere e lo fanno secondo l'ordine dell'indirizzo.
Essendo prestabilito l'ordine, non avverranno collisioni.
Al termine della fase di trasmissione, ci sarà un nuovo periodo di contesa dove si ripeterà il processo di "prenotazione".
Ogni stazione può porre il proprio bit "pronto" solo nel suo intervallo e può farlo se ha effettivamente il frame preparato.
Se la stazione diventa pronta ma il suo intervallo è passato, dovrà attendere la prossima fase di contesa.

Questo tipo di protocolli sono detti protocolli a prenotazione.

\paragraph{Conteggio binario}
Il problema del metodo della mappa è la richiesta del bit di controllo: se ci sono tante stazioni, sono richiesti tanti bit (uno per stazione).
Un'alternativa è quella di utilizzare indirizzi binari per le stazioni.
Una stazione che vuole trasmettere lo comunica agli altri tramite il proprio indirizzo, spedito sotto forma di stringa partendo dal bit più a sinistra.
Dato che gli indirizzi sono tutti lunghi uguali, i bit nella stessa posizione sono elaborati tramite OR: 
se il risultato è un 1, tutte le stazioni con bit 0 si arrendono;
poi si passa al confronto del bit successivo con le stazioni rimanenti e se risulta uno 0 si passa al bit successivo,
se un 1, si arrendono quelle con lo 0 e si prosegue, finché non rimane una stazione e sarà quella che spedirà il frame.
Questo protocollo è detto conteggio binario e chiaramente introduce un livello di priorità tra le stazioni (max priorità l'indirizzo più grande),
che in base alle situazioni può essere vantaggioso o meno.

Al momento è un protocollo inutilizzato nonostante la semplicità.

\subsubsection{Protocolli a contesa limitata}
Meglio i protocolli a contesa o senza collisioni?
Il primo è preferibile per il basso ritardo quando il carico è leggero, ma al crescere del carico peggiora l'efficienza.
Il secondo funziona all'opposto.

I protocolli a contesa limitata si pongono a metà strada tra i due.
Questi protocolli dividono le stazioni in gruppi (anche non esclusivi) e in ogni gruppo le stazioni competono tra loro per un determinato intervallo:
il gruppo 0 per l'intervallo 0, il gruppo 1 per l'1, e così via.
Il trucco sta nel ripartire le stazioni nel modo ottimale.
La soluzione migliore è quella di assegnare dinamicamente in base al carico.

\paragraph{Adaptive Tree Walk}
Questo metodo sfrutta gli alberi binari.
Nel primo intervallo di contesa (0), tutte le stazioni tentano di acquisire il controllo. 
Se non riescono perché c'è una collisione, le stazioni vengono divise in due gruppi e 
nel successivo intervallo (1) provano ad ottenere il canale quelle del secondo nodo.
Se la trasmissione avviene con successo, l'intervallo successivo (2) va al nodo 3, altrimenti si suddivide il 2 in due sottogruppi e così via.

A carico elevato si dovrebbe iniziare già a livelli più bassi dell'albero.

\subsubsection{Protocolli LAN Wireless}\label{problemiStazioni}
Esempio di LAN wireless: un ufficio con stazioni base (o access point) sparsi e collegate tra loro tramite cavi.
Regolando la potenza del segnale attorno a 3-4 metri, si ottiene una situazione dove le stanze sono considerabili come delle celle.
Una cella ha un unico canale che copre tutta la banda ed è disponibile a tutte le stazioni.
Nel caso della LAN wireless il problema non è il trasmettitore bensì il ricevente.
Infatti considerando delle stazioni con portata limitata, dove A raggiunge B, B raggiunge C, ma A e C non comunicano direttamente,
potrebbe accadere che A cerca di comunicare con B, mentre C che non sente A pensa sia tutto libero e cerca di comunicare con B. 
Il risultato è che A e C creeranno una collisione, danneggiando i frame.
Questo \textbf{problema} è detto \textbf{della stazione nascosta}.
All'inverso, se B trasmette ad A e C controlla la banda portante per trasmettere a D, siccome rileva la trasmissione di B pensa che anche D sia occupato.
In questo caso di dice \textbf{problema della stazione esposta}. 

\paragraph{MACA e MACAW}
MACA (Multiple Access with Collision Avoidance) è uno dei primi protocolli per LAN wireless.
Il trasmettitore invita il ricevitore ad inviare un piccolo frame per indicare che alle stazioni vicine che è occupato, 
in modo da poter inviare subito dopo il frame dati effettivo.
In particolare:
A vuole trasmettere a B; manda un RTS (request to send) a B che contiene la lunghezza del frame dati in arrivo;
B risponde con un CTS (clear to send) che contiene la lunghezza dei dati, copiata dal RTS.
In questo modo A può mandare il frame quando ha ricevuto la CTS.

Le stazioni vicine ad A che ricevono RTS devono stare in silenzio per attendere che CTS arrivi ad A.
Viceversa, le stazioni vicine a B che ricevono CTS devono stare in silenzio per permettere la trasmissione del frame dati.

In caso di collisione verrà effettuato un secondo invio con intervallo di tempo casuale.

Esiste poi la versione migliorata MACA per wireless, ovvero MACAW.
In questo protocollo è stato introdotto un frame ack dopo la trasmissione dati avvenuta con successo,
ed è stata aggiunta la possibilità di bloccare le stazioni vicine che stanno mandando un RTS quando ce n'è già uno in viaggio verso la stessa destinazione.

\subsection{Ethernet} % 4.3 TUTTO tranne 439
Gli standard più importanti:
\begin{itemize}
\item 802.3 - Ethernet
\item 802.11 - LAN wireless
\end{itemize}
Hanno sottostrati fisici e sottostrati MAC differenti, ma hanno la stessa interfaccia verso lo strato network.

\subsubsection{Cablaggio Ethernet}
Ethernet è il nome del cavo utilizzato per 802.3 e ne esistono 4 tipi.
Il modello più vecchio è il 10base5, o thick Ethernet.
Il cavo è piuttosto spesso e le connessioni sono realizzate mediante spine a vampiro (?).
Il nome indica che opera a 10Mps con segnali a banda base e sopporta segmenti lunghi fino a 500 metri.

Il secondo cavo è 10Base2 o thin Ethernet;
come per il precedente, dal nome si sa che opera a 10Mps con segnali a banda base e sopporta segmenti lunghi fino a 200 metri circa.
In questo caso le connessioni sono fatte mediante connettori BNC che formano giunzioni a T; sono più facili da usare e più affidabili.
Questo cavo è economico, semplice da installare, ma supporta solo segmenti di 185 metri e un massimo di 30 macchine.

I guasti sono un problema per le connessioni cablate; per questo esistono tecniche per rintracciare i guasti,
come la TDR (Time Domain Reflectometry), la quale usa un impulso per vedere se è tutto apposto.
Se ci sono guasti o ostacoli si genera un eco che torna indietro e cronometrando il tempo di ricezione dell'eco è possibile identificare la distanza dell'origine dell'eco.

Dal problema del tracciamento guasti, si è passati all'utilizzo di hub; le stazioni sono collegate agli hub tramite doppini non condivisi.
Questo hub non elabora il traffico.
Questa configurazione permette di aggiungere e rimuovere stazioni facilmente ed ha una facile individuazione le interruzioni della linea.
Tuttavia i cavi che partono dall'hub possono raggiungere solo i 100/200 metri.
Nonostante questo è diventato uno standard per la facilità di gestione e per l'uso dei cablaggi presistenti.
Lo schema presentato è detto 10Base-T.

Un altro tipo di cavi è il 10Base-F che utilizza le fibre ottiche.
Questa alternativa è costosa a causa del prezzo di connettori e terminatori, ma è immune alle interferenze e permette i collegamenti a lunghe distanze (1km).

\subsubsection{La codifica Manchester}
Utilizzare una codifica binaria corretta è un problema non indifferente, perchè vanno scelti valori che non possono creare ambiguità.
Deve essere ben chiaro quando si ha l'inizio e la fine di un bit.
Esistono due tecniche: la codifica di Manchester e la codifica di Manchester differenziale.

La prima utilizza un periodo di bit diviso in due intervalli uguali, dove l'1 binario è identificato da un picco di tensione nel primo intervello e un livello basso nel secondo intervallo.
Lo zero binario è l'opposto.
Il cambio di tensione in mezzo al periodo di bit permette al ricevitore di sincronizzarsi con il trasmettitore;
tuttavia occupa il doppio della banda rispetto alla codifica elementare.

La codifica di Manchester differenziale è un'alternativa dove un 1 binario è identificato dall'assenza di cambio di tensione tra un periodo di bit e un altro,
mentre se avviene una transizione identifica uno zero.
Questa codifica è più complessa ma ha una maggiore immunità ai rumori.
Tuttavia Ethernet usa la Manchester base.

\subsubsection{Il protocollo del sottostrato MAC Ethernet}
Esistono due varianti di frame: quello DIX Ethernet e quello dello standard IEEE.
Lo standard apporta solo delle piccole modifiche a due campi (type diventa lenght, preamble viene ridotto),
tuttavia DIX aveva già preso piede a tal punto che pochi erano disposti a passare al nuovo standard.
Tuttavia il campo type prima della creazione dello standard era sempre maggiore di 1500,
quindi se è più corto viene considerato come campo lenght dello standard.
Con questo compromesso si riuscì ad usare entrambi i frame.

Più nello specifico, il frame DIX è composto da:
\begin{itemize}
    \item preamble
    \item indirizzo di destinazione
    \item indirizzo d'origine
    \item type
    \item dati
    \item riempimento
    \item checksum
\end{itemize}
Preamble è di 8 byte e contiene lo schema di bit 10101010.

I due indirizzi sono di 6 byte. In particolare l'indirizzo di destinazione ha il bit più a sinistra che identifica un indirizzo ordinario (0) o un indirizzo di gruppo (1).
Con l'indirizzo di gruppo è possibile far ascoltare a molte stazioni un singolo indirizzo e tutte lo ricevono (trasmissione multicast).
Se l'indirizzo è composto da soli 1 è riservato alla trasmissione broadcast.

Type di 2 byte indica al ricevitore cosa fare con quel frame.

I dati raggiungono i 1500 byte di massimo; è inoltre necessario che il frame in totale sia lungo almeno 64 byte, per distinguere dai frame danneggiati.
Inoltre la lunghezza minima serve per dare il tempo di impedire la trasmissione di un frame quando viene rilevata la possibilità di collisione.
Maggiore è la velocità della rete, maggiore deve essere la lunghezza minima del frame.

Il checksum di 4 byte e contiene il checksum CRC per rilevare (ma non correggere) gli errori.

\subsubsection{Algoritmo di backoff esponenziale binario}
L'algoritmo di backoff esponenziale binario è utilizzato per gestire l'attesa casuale dopo una collisione. 
\`E stato scelto perché si adatta in base al numero di stazioni che tentano di trasmettere.
L'algoritmo assicura un basso ritardo quando si hanno poche collisioni e una gestione accettabile quando avvengono molte collisioni. 

L'algoritmo procede in questo modo: 
dopo una collisione il tempo viene diviso in intervalli lunghi quanto il tempo di propagazione di andata e ritorno nel caso peggiore (2t);
ogni stazione aspetta 0 o 1 intervalli temporali prima di riprovare;
se due stazioni collidono perché usano lo stesso intervallo, aumenta la scelta degli intervalli casuali (0, 1, 2, 3);
ad ogni passo si continua ad aumentare il numero di intervalli d'attesa, che andrà da 0 a $2^i-1$, dove i indica il numero di collisioni.
L'algoritmo ha come tetto massimo 10 collisioni, per un totale di 1023 intervalli. Se ci sono ancora collisioni, a 16 viene lanciato un errore.

\subsubsection{Prestazioni di Ethernet}
[...]

\subsubsection{Ethernet commutata}
Al crescere del numero di stazioni, il traffico aumenta e l'aumento della velocità non basta a risolvere il problema.
La soluzione è lo switch (commutatore), che ha da 4 a 32 schede di linea, le quali hanno da 1 a 8 connettori, di solito collegati a doppini 10base-T verso i computer.

Una stazione che vuole trasmettere un frame Ethernet, prima invia un frame standard allo switch, il quale controlla se la destinazione è collegata alla stessa scheda di linea.
Se è nella stessa scheda, passa il frame direttamente, altrimenti viene passato alla scheda corretta.

Problema: se due macchine trasmettono sulla stessa scheda? \\
Ci sono due possibilità.
Se la scheda è costruita con le porte che formano una LAN locale, le collisioni vengono rilevate e gestite come in una rete CSMA/CD.
In questo modo è possibile una sola trasmissione per scheda, ma più trasmissioni in parallelo su schede diverse.
Ogni scheda costituisce il proprio dominio di collisione.

La seconda possibilità prevede un buffer per ogni porta che memorizza i frame in arrivo.
Quando ottiene tutto il pacchetto può controllare se il destinatario è nella stessa scheda o se serve spostarlo nella scheda appropriata.
Ogni porta è un dominio di collisione separato, quindi non ci sono collisioni.

\subsubsection{Fast Ethernet}
Nonostante l'aumentare della velocità, l'utente non è mai soddisfatta. 
Vennero allora proposte due LAN ottiche su topologie ad anello: FDDI e Fibre Channel.
Non ebbero molto successo, ma aprirono la strada ad una maggiore velocità.
Lo standard infatti decise di mantenere il 802.3 ma con velocità superiore, in modo da mantenere la compatibilità con le LAN esistenti.
Venne così creato Fast Ethernet (o anche il poco usato 802.3u). 
Di fatto la differenza sta nel tempo di bit ridotto da 100 a 10 nanosecondi.
Per questa versione era tollerata solo l'architettura 10base-T. Con quali cavi?\\
Il doppino cat-3 era una prima scelta data la sua diffusione, tuttavia non riusciva a trasportare 200 Megabaud (ovvero 100 Mbps con codifica Manchester) per 100 metri, come richiesto dal 10Base-T.
Venne quindi permesso il cat-3, ma consigliato il cat-5 e la fibra ottica. Abbiamo quindi, rispettivamente, 100Base-T4, 100Base-TX, 100Base-FX, con le ultime due full duplex a 100Mbps.

Lo schema Cat 3 necessita di 4 doppini: uno in ricezione, uno in trasmissione e gli altri due si adeguano alla direzione di trasmissione.
I segnali trasmessi sono ternari, cioè possono contenere 0,1 o 2. 
In questo modo si riesce a raggiungere la velocità obiettivo di 100 Mbps. Questo schema è chiamato 8B/6T.

Il modello basato sul Cat 5 è più semplice ed utilizza solo due doppini, uno in ricezione e uno in trasmissione.
In questo caso viene usato uno schema chiamato 4B/5B.
Questo sistema è full duplex, ovvero le stazioni possono trasmettere e ricevere contemporaneamente a 100 Mbps.

Il modello che utilizza la fibra ottica usa due cavi multimodali, uno per direzione.
Quindi il sistema è full duplex con 100 Mbps in ogni direzione e può raggiungere 2 km tra stazione e hub.
Per i modelli 100Base-T è possibile utilizzare sia switch che hub;
invece il modello a fibra ottica consente solo lo switch dove ogni cavo collegato crea un dominio di collisione verso se stesso. 

\subsubsection{Gigabit Ethernet}
Dopo la Fast Ethernet iniziarono a lavorare su una Ethernet ancora più veloce, ovvero la gigabit Ethernet.
Anche in questo caso si puntava ad avere una trasmissione più veloce mantenendo la compatibilità con le versioni precedenti.
Le configurazioni gigabit Ethernet sono tutte punto a punto, di conseguenza ogni stazione è collegata ad un solo hub e switch.
Sono supportate sia il full duplex che l'half duplex.
Quella normale è la full duplex, utilizzata quando c'è uno switch centrale collegato al computer alle stazioni del perimetro.
Tutte le linee hanno buffer e non è possibile avere quel collisioni, quindi non è necessario controllare se il canale è già utilizzato e di conseguenza non si utilizza il protocollo CSMA/CD. 
Si utilizza la half duplex quando i computer sono collegati ad un hub; 
in questo caso le collisioni sono ancora possibili quindi si utilizza il protocollo CSMA/CD.
Data la velocità di trasmissione, un frame è trasmesso così velocemente da limitare la distanza a 25 metri.
Questo è inaccettabile, di conseguenza sono state aggiunte due funzionalità dallo standard.
La prima è chiamata \textbf{Carrier extension}, aggiunge i dati di riempimento dopo il frame normale e viene rimosso quando viene ricevuto. 
Essendo una modifica hardware non sono richiesti cambiamenti al software. 
La seconda funzionalità è detta \textbf{Frame bursting} e permette di inviare una sequenza concatenata di più frame in una sola trasmissione;
se non viene raggiunta la dimensione di 512 Byte, viene compensata dal metodo sopracitato;
in questo modo si arriva ad un raggio di 200 metri un valore già più accettabile.
Esistono 4 tipi di configurazione che utilizzano cavi diversi: 1000Base-SX/LX/CX/T, le prime due con fibra, le ultime due con diversi tipi di doppini.

\subsubsection{Retrospettiva su Ethernet}
Punti vincenti: semplicità e flessibilità.

Semplice si traduce in affidabile, economico e facile da mantenere.
Flessibile in quanto funziona con TCP/IP ed è in grado di evolversi dove serve.

\subsection{LAN Wireless} % 4.4 TUTTO tranne 444

\subsubsection{La pila di protocolli 802.11}
I protocolli utilizzati dalle varianti 802 sono tutti simili.
Lo strato fisico è simile a quello OSI, lo strato datalink è suddiviso in due o più sottostrati.
In 802.11 il sottostrato MAC gestisce l'allocazione del canale, il sottostrato LLC nasconde le differenze tra le varianti di 802 in modo da renderle indistinguibili allo strato network.

\subsubsection{Lo strato fisico di 802.11}
[...]

\subsubsection{Il protocollo del sottostrato MAC di 802.11}\footnote{NdR: parte poco chiara e incompleta}
Il sottostrato MAC di 802.11 si differenzia leggermente da quello di 802.3, in quanto l'ambiente wireless complica le cose.
Per esempio ci sono i problemi della stazione esposta e della stazione nascosta (§\ref{problemiStazioni}).
Inoltre le trasmissioni sono quasi tutte half duplex, quindi non può trasmettere e rimanere in ascolto sulla stessa frequenza contemporaneamente.
Per questi motivi non viene utilizzato CSMA/CD, bensì una variante detta CSMA/CA, ovvero Collision Avoidance.

In particolare, 802.11 supporta due modalità operative: 
DCF (Distributed Coordination Function), che non utilizza nessun controllo centrale,
e PCF (Point Coordination Function), dove invece la stazione base controlla l'intera cella.

Quando viene usato DCF, si usa CSMA/CA che controlla sia il canale fisico, sia quello virtuale.

Questo protocollo rileva la portante prima della trasmissione e usa il backoff esponenziale dopo le collisioni;
a differenza di CSMA/CD, il backoff di partenza è casuale e non attende una collisione per essere settato.


Le reti wireless sono rumorose e inaffidabili.
Per risolvere i problemi dei canali rumorosi, 802.11 ammette la frammentazione dei frame.
Ogni frammento ha un proprio checksum, sono numerati e ricevono l'ack secondo stop-and-wait.
Quando viene acquisito un canale, più frammenti possono essere mandati insieme in un burst.
Questa tecnica aumenta l'efficienza perché permette di reinviare solo i frammenti danneggiati.

Nel caso di PCF, l'ordine di trasmissione è controllato dalla centrale base, quindi non avvengono collisioni.
La stazione base trasmette in broadcast un frame di segnalazione con il quale indica i parametri di sistema e invita le stazioni a registrarsi al servizio di interrogazione.

Dato che la batteria può essere un problema nei dispositivi wireless, la stazione base può obbligare quella mobile ad andare in sospensione.
Mentre è disattivata la stazione mobile, la base deve memorizzare in un buffer i dati diretti alla stazione sospesa.
Viene riattivata dall'utente o dalla stazione base.

\subsubsection{Servizi}
Lo standard definisce 9 servizi che ogni LAN wireless deve fornire, 5 di distribuzione e 4 di stazione.
Quelli di distribuzione riguardano l'appartenenza alla cella e l'interazione con le altre celle, mentre quelli di stazione si occupano delle attività dentro la cella.
I servizi sono:
\begin{itemize}
\item Associazione - utilizzato dalle stazioni mobili per collegarsi alle stazioni base
\item Separazione - si interrompe la relazione tra le due stazioni
\item Riassociazione - la stazione mobile cambia la stazione base preferita (es: per cambiare cella)
\item Distribuzione - indica come saranno instradati i frame verso la destinazione
\item Integrazione - traduce dal formato 802.11 al formato richiesto dalla destinazione
\item Autenticazione - solo le stazioni autenticate possono trasmettere i dati
\item Invalidamento - invalida una stazione precedentemente autenticata che vuole lasciare la stazione
\item Riservatezza - le informazioni devono essere cifrate
\item Trasferimento dati - per il trasferimento
\end{itemize}


\subsection{Commutazione nello strato data link} % 4.7 TUTTO tranne 476
\`E possibile connettere più LAN tramite dispositivi detti bridge, che operano nello strato data link.
I bridge esaminano gli indirizzi dello strato datalink per l'instradamento senza esaminare il carico utile, permettendo di trasportare diversi tipi di pacchetto.
I router all'opposto instradano in base agli indirizzi del pacchetto.

Alcuni motivi per creare LAN differenti che vengono poi connesse tra loro sono:
\begin{itemize}
\item rendere indipendenti le LAN dei dipartimenti diversi
\item collegare aziende su edifici diversi, lontani tra loro
\item suddividere una LAN logica in più LAN fisiche per gestire meglio il carico
\item per collegare più LAN suddivise per i limiti di distanza (es: 2,5 Km in caso di Ethernet)
\item per garantire una maggiore affidabilità in caso di nodi difettosi
\item per ottenere una maggiore sicurezza dell'azienda
\end{itemize}

\subsubsection{Bridge tra due 802}
Come funziona un bridge?
Un pacchetto dallo strato di rete viene passato allo strato LLC che aggiunge l'intestazione relativa,
poi passa al sottostrato MAC dove viene aggiunta la sua intestazione, per esempio 802.11;
infine giunge allo strato fisico fino ad arrivare alla stazione base, dove viene rilevata la richiesta di passaggio tra due LAN di tipi diversi.
Il passaggio avviene tramite il bridge, che "traduce" il frame risalendo gli strati del bridge, fino a riscendere per passare allo strato fisico della seconda LAN.
Purtroppo non è così banale, in quanto ci sono diversi problemi.
Ad esempio i formati dei frame differiscono, richiedendo una conversione particolare;le due LAN possono trasmettere a velocità diverse;
inoltre c'è la diversa lunghezza dei frame in base al tipo di 802; un'altra questione è la sicurezza, in quanto alcune versioni supportano la crittografia, mentre altri no; infine la qualità del servizio, che viene garantito in modi diversi.

\subsubsection{Internetworking locale}
Come già accennato, i bridge possono essere usati per connettere LAN anche dello stesso tipo.
In questo caso, i bridge accettano tutti i frame in arrivo, ma scartano quelli che appartengono già alla stessa LAN,
ovvero quei frame che non hanno bisogno di attraversare il bridge per cambiare LAN.
Nel decidere dove e se inoltrarli, il bridge utilizza l'indirizzo di destinazione e lo confronta con una tabella delle destinazioni.
Appena creata la connessione le tabelle sono vuote e il bridge manda il frame a tutte le LAN tranne a quella di input;
con il tempo riesce a riempire la tabella con gli indirizzi corretti e potrà inviare solo alla LAN che contiene la stazione richiesta.
Questo è detto algoritmo di apprendimento all'indietro.
In particolare la costruzione avviene salvando gli indirizzi della sorgente, memorizzando in che LAN si trova.

\subsubsection{Bridge spanning tree}
Per aumentare l'affidabilità è possibile collegare più LAN con bridge paralleli, tuttavia questo crea anelli nella topologia.
Per evitare il problema, viene costruito uno spanning tree, un grafico ad albero che evita i percorsi ad anello.

\subsubsection{Bridge remoti}
Delle LAN distanti possono essere connesse tra loro tramite bridge remoti,
ovvero una coppia di bridge che sono in comunicazione tra loro tramite una linea punto a punto.

\subsubsection{Ripetitori, hub, bridge, switch, router, gateway}
Nonostante la funzione simile, vale la pena confrontare ripetitori, hub, bridge, switch, router, gateway.
Innanzitutto, operano su strati diversi:
ripetitori e hub nel fisico, bridge e switch nel data link, router nel network, gateway nel trasporto e applicazioni (due tipi diversi).

\newpage
\section{Capitolo 5 - Strato network}
Lo strato network si occupa del trasporto dei pacchetti da origine a destinazione, passando tra i vari router, a differenza di quello data link che si occupa dei frame.
Lo strato deve scegliere il percorso migliore, evitando sovraccarichi o sprechi, e deve gestire i problemi che derivano dalla presenza della sorgente e della destinazione su diverse reti. 

\subsection{Architettura dello strato network} % 5.1

\subsubsection{Commutazione di pacchetto store-and-forward}
Dati due host collegati a diversi router, quando devono comunicare viene inviato un pacchetto al router più vicino, 
il quale lo slava finché non arriva completo.
Dopo che è stato verificato il checksum, viene inviato al router successivo secondo un certo percorso fino ad arrivare alla destinazione.
Questo meccanismo è detto commutazione store-and-forward.

\subsubsection{Servizi forniti allo strato di trasporto}
I servizi forniti allo strato di trasporto devono essere indipendenti dalla tecnologia del router e lo strato di trasporto non deve essere influenzato dal numero, tipo e topologia dei router.

La questione principale è: dovrebbe essere un servizio senza connessione o orientato alla connessione?

\paragraph{Implementazione del servizio senza connessione}
In questo caso i pacchetti sono inoltrati e instradati indipendentemente l'uno dagli altri.
I pacchetti sono chiamati datagrammi e la sottorete è detta a datagrammi.

Dato un messaggio da spedire tra due host, il messaggio all'origine viene diviso in pacchetti se troppo lungo e vengono inviati uno alla volta al primo router.
Il router ha una tabella che elenca dove vanno inviati i pacchetti in base alla destinazione.
Come già detto, vengono archiviati per calcolare il checksum, poi ogni pacchetto viene inoltrato al router successivo secondo quanto indicato nella tabella.
Il router di passaggio indicato nella tabella può variare, per esempio in base al traffico rilevato; 
l'aggiornamento della tabella avviene tramite gli algoritmi di routing.
Passando da router a router i pacchetti giungeranno l'host di destinazione che potrà ricomporre il messaggio.

\paragraph{Implementazione del servizio orientato alla connessione}
Nel servizio orientato alla connessione va stabilito il percorso prima di inviare i pacchetti.
La connessione è detta CV (circuito virtuale) e la sottorete a circuito virtuale.

A differenza del precedente, quando è stabilita la connessione il percorso è fisso e salvato nelle tabelle dei router.
Il percorso è usato per tutti i pacchetti del messaggio.
Ogni pacchetto ha un identificatore che indica il CV a cui appartiene. 
Alla fine della connessione, viene chiuso il CV.

Quando due host hanno lo stesso identificatore di connessione e giungono ad un router, quest'ultimo ha il potere di cambiare il numero di connessione del traffico in uscita per risolvere il conflitto.

\subsubsection{Sottoreti a circuito virtuale vs a datagramma}
Quale dei due servizi è meglio?\\
Nel CV, un pacchetto può usare il numero di circuito al posto dell'indirizzo di destinazione, che può portare ad un risparmio di banda.

A livello di qualità del servizio, CV offre un servizio migliore perché definendo a priori strada e banda, ha tutto prefissato ed evita problemi di congestione.

Se un router va in crash, tutti i CV che passano per quel router vengono terminati, mentre nel datagramma viene cambiato il percorso.

\subsection{Algoritmi di routing} % 5.2
Gli algoritmi di routing definiscono l'instradamento dei pacchetti.
Nella sottorete a datagrammi la decisione va fatta per ogni pacchetto, mentre nel CV viene fatto solo nel momento in cui viene stabilita la connessione (in questo caso detto anche routing di sessione).

Va distinto l'inoltro e l'instradamento:
il primo è la gestione del pacchetto in un router secondo la tabella di routing, in cui viene cercata la linea adatta; 
il secondo è il processo di riempimento e aggiornamento della tabella.

Un algoritmo di routing dovrebbe avere le seguenti proprietà:
precisione, semplicità, robustezza, stabilità, imparzialità e ottimizzazione.

Gli algoritmi si possono distinguere in due classi:
\begin{itemize}
\item adattivi, cambiano le decisioni in base al traffico e alle modifiche della topologia (datagrammi)
\item non adattivi, non basano le decisioni sul traffico o sulla topologia; il percorso viene calcolato in anticipo ed è fisso (CV)
\end{itemize}

\subsubsection{Principio di ottimalità}
\`E possibile calcolare il percorso più breve con il principio di ottimalità.
Dal principio deriva l'albero sink tree, che rappresenta le distanze tra i router in base alla distanza dei nodi e delle foglie dalla radice, che rappresenta il router di partenza.
Nella pratica il sink tree può non essere la soluzione "definitiva" a causa di possibili imprevisti, guasti, ecc.

\subsubsection{Routing basato sul percorso più breve}
Un tipo di routing è quello basato sul cammino minimo.
Ci sono diversi algoritmi per calcolarlo, per esempio quello di Dijkstra.
Questo algoritmo è statico.

\subsubsection{Flooding}
Il flooding è un altro algoritmo statico.
Questo algoritmo invia il pacchetto in tutte le linee tranne quella di arrivo.
Chiaramente genera molti pacchetti duplicati, che sono da gestire.
Una possibilità è quelli di usare un contatore di salti che viene decrementato ad ogni inoltro: se arriva a zero viene scartato.
Il valore iniziale dovrebbe essere dovrebbe essere la distanza origine-destinazione, se conosciuta.
In alternativa è possibile tracciare con un numero di sequenza il passaggio dei pacchetti: quando passa un pacchetto viene salvato in una tabella il numero di sequenza, in modo da scartare le sue successive occorrenze.

Una variante è il flooding selettivo, dove il pacchetto viene mandato solo nelle linee nella direzione "giusta"

Per quanto sia uno spreco, il flooding può essere ottimo per la sua robustezza in determinati contesti, ad esempio quello militare dove si potrebbero perdere molti router in poco tempo;
oppure se va aggiornato un sistema di database distribuito;
un'altra possibilità è nelle rete wireless, dove tutti i messaggi possono essere ricevuti da tutte le altre stazioni.

\subsubsection{Routing basato sul vettore delle distanze}
Gli algoritmi basati sul vettore delle distanze sono di tipo dinamico.
Spesso prendono il nome dell'algoritmo che usano, come Bellman-Ford.
Ciascun router ha una tabella di routing indicizzata da ogni router della sottorete, con una voce per ogni router.
Questa indica la linea preferita per quella destinazione e una metrica per quella destinazione: distanza in salti, lunghezza della coda, tempo come ritardo.
Questi dati sono continuamente aggiornati scambiando informazioni con i router vicini.

Il principale difetto di questo metodo è il fatto che per raggiungere la risposta corretta per il percorso migliore, ci può mettere molto tempo.
In particolare il problema è il conteggio all'infinito\footnote{NdR: spiegato male}, ovvero quando le tabelle continuano a venire aggiornate, ma la distanza incrementa sempre di più.

Inoltre l'algoritmo non tiene conto della banda.

Per questi due problemi, venne rimpiazzato dal routing basato sullo stato dei collegamenti.

\subsubsection{Routing basato sullo stato dei collegamenti} % no elaborazione nuovi percorsi
Un altro algoritmo dinamico è quello basato sullo stato dei collegamenti,
Si basa su 5 punti che deve fare ogni router:
\begin{itemize}
\item scoprire i vicini e i loro indirizzi di rete
\item misurare il ritardo o il costo di ogni vicino
\item costruire un pacchetto con tutte le info raccolte
\item inviare il pacchetto agli altri
\item elaborare il percorso più breve verso tutti gli altri router
\end{itemize}
Innanzitutto il router invia un pacchetto HELLO per capire chi ha nelle vicinanze e in router devono rispondere identificandosi con un nome unico globalmente.
Se tra dei router c'è una LAN, questa viene considerata come un nodo artificiale della rete.

Per misurare il ritardo dei router vicini viene usato un pacchetto ECHO, al quale devono rispondere subito.
Tempo di andata + ritorno diviso due e si ottiene il ritardo stimato.

Una volta ottenute le info di scambio, viene creato il pacchetto dove è indicata l'identità del trasmittente, un numero di sequenza, un'età e la lista dei vicini con il relativo ritardo.
I pacchetti possono essere costruiti periodicamente o quando accade un evento significativo.

La distribuzione di questi pacchetti è un punto cruciale, perché può creare inconsistenza.
Viene utilizzato il flooding con numero di sequenza che viene incrementato per ogni nuovo pacchetto inviato.
I router tengono traccia delle coppie router sorgente-sequenza;
se arriva un pacchetto di info, viene confrontato: se è nuovo viene inoltrato a tutte le linee tranne quella d'arrivo, se è duplicato o vecchio i dati vengono scartati.\\
I problemi di questo metodo sono vari.
I numeri di sequenza devono essere lunghi per evitare ripetizioni.
Se un router si blocca, perde traccia dei numeri di sequenza.
Se il numero di sequenza è danneggiato, possono essere saltati altri pacchetti leciti (per esempio arriva un numero di sequenza più grande).

\subsubsection{Routing gerarchico}
Le tabelle crescono man mano che aumenta la rete, il che consuma memoria, cpu e banda.
Si deve quindi utilizzare un routing gerarchico, dove i router sono divisi in regioni:
i router conoscono solo le info del routing all'interno della regione.
Per reti grandi, le regioni non bastano e vengono fatti altri raggruppamenti a livelli.
All'interno di una regione è presente un router che fa da ponte e riesce a comunicare con altre regioni.

Il risparmio di spazio tutta via aumenta la lunghezza dei percorsi.

\subsubsection{Routing broadcast}
A volte può servire trasmettere un pacchetto a molti/tutti host.
Per fare ciò si usa una trasmissione broadcast. 
Per attuare questa trasmissione ci sono diversi modi:
inviare a tutti un pacchetto distinto, sprecando banda;
utilizzo del flooding, ma genera troppi pacchetti e spreca banda.

Una terza tecnica è il multidestination routing, un algoritmo nel quale un pacchetto ha la lista di tutte le destinazioni.
Un router riceve un pacchetto, controlla la lista, duplica il pacchetto e lo inoltra in base a ogni destinazione indicata.
Ad ogni inoltro le destinazioni vengono consumate, finchè non diventa un pacchetto normale con un'unica destinazione.

Una quarta possibilità usa lo spanning tree, un albero che contiene tutti i router, senza cicli.
In questo modo un router sa quali linee appartengono all'albero e può inviare il pacchetto in tutte le linee dello spanning tree tranne quella di ingresso.
Ottimo utilizzo della banda e poche copie inutili, ma deve conoscere lo spanning tree (non sempre possibile).

L'ultimo algoritmo usa il reverse path forwarding:
quando un pacchetto arriva ad un router, viene controllato se è arrivato dalla linea usata di solito per comunicare con il sorgente;
se si, significa che è già nel percorso migliore e che è la prima copia, e viene inoltrato in tutte le linee tranne quella d'arrivo, se non lo è viene scartato.
Il metodo è facile da implementare e abbastanza efficiente.

\subsection{Algoritmi per il controllo della congestione} % 5.3
La congestione è un problema che si presenta quando ci sono troppo pacchetti in una sottorete, che sorpassano la capacità di trasporto.
Di solito i pacchetti in più vengono persi.

La congestione può verificarsi quando si crea coda ma non c'è abbastanza memoria per mantenere tutti in coda, oppure se il processore è troppo lento e non riesce a gestire il carico.

C'è differenza tra controllo del flusso e controllo della congestione.
Quello della congestione punta ad assicurare che la sottorete sia in grado a trasportare il traffico immesso, mentre quello del flusso gestisce il traffico punto a punto tra sorgente e destinazione.

\subsubsection{Principi del controllo della congestione}
Al problema del controllo ci sono due gruppi di possibili soluzioni:
quelle a ciclo aperto, che puntano su una soluzione che una volta in atto non esegue correzioni in corsa e non tiene conto dello stato attuale della rete;
le soluzioni a ciclo chiuso controllano dinamicamente il sistema per verificare quando e dove si presenta la congestione, in modo da eseguire le manovre correttive nei punti dove è possibile farlo.

Nel ciclo chiuso ci sono diversi modi per valutare la congestione: numero di pacchetti scartati, lunghezza della coda, ritardo, pacchetti ritrasmessi perché scaduti, ecc.
Poi comunica il problema; la cosa più ovvia è riferirlo alla sorgente (ma esistono altre alternative).
La soluzione che può venire intrapresa è ridurre il carico o aumentare le risorse.

\subsubsection{Prevenire la congestione}
I metodi a ciclo aperto servono a ridurre la possibilità di congestione, non a risolverla.
Questi lavorano su più livelli: trasporto, network, data link.

Lo strato data link deve gestire principalmente la ritrasmissione dei pacchetti scaduti e gli acknowledgement. 
Per esempio la ripetizione selettiva è migliore del metodo go-back-n.
Gli acknowledgement possono generare traffico aggiuntivo, quindi va gestito.

Lo strato network è influente nella scelta tra sottorete a circuito virtuale o a datagrammi, in quanto molti algoritmi di controllo della congestione funzionano solo nella prima.

\subsubsection{Controllo nelle sottoreti a circuito virtuale}
Una tecnica per gestire una congestione dinamicamente è il controllo di ammissione, quale, quando è rilevata la congestione, impedisce di impostare nuovi circuiti virtuali.
Semplice e facile da implementare.
Un'alternativa è creare circuiti virtuali che aggirano la congestione, se possibile.
Un'altra possibilità è quella di prenotare le risorse quando viene stabilito il circuito virtuale, in modo da essere sicuri che vada a buon fine;
se non sono disponibili, aspetta.

\subsubsection{Controllo nelle sottoreti a datagrammi}
Un router può tenere sotto controllo le linee di output, associando ad ogni linea una variabile da 0 a 1.0 che indica quanto è utilizzata la linea.
Quando il valore si avvicina alla soglia, il router controlla se il pacchetto in arrivo è diretto ad una linea in allarme, se lo è esegue le manovre correttive.

\paragraph{Choke packet}
Questo metodo invia un pacchetto (choke packet appunto) al sorgente per farlo rallentare, contenente la destinazione.
Il pacchetto originale viene etichettato per indicare che ha generato un choke packet, poi viene inoltrato.
Il sorgente deve rallentare di una certa percentuale e per un po' deve ignorare altri choke packet, dato che è probabile che ne arrivino altri da pacchetti instradati verso la stessa destinazione.
La velocità di solito viene dimezzata per ogni choke packet e ripristinata successivamente gradualmente.

Il principale problema è la gestione a lunghe distanze o ad alte velocità: il choke packet provoca una reazione lenta nel sorgente in questi casi.
La soluzione è la variante hop-by-hop, nel quale il choke packet ha effetto ad ogni salto all'indietro, rallentando ogni router nel percorso e dicendogli di salvare nel buffer i dati in arrivo.
In questo modo i router successivi hanno un sollievo dal punto di vista del carico, mentre quello corrispondente al punto dove è arrivato il choke packet sposta il peso delle operazione sul buffer, che mette in coda.
In questo modo mentre il choke packet risale il percorso fino al sorgente, si ha un sollievo graduale e permette di non perdere nessun pacchetto. 
Chiaramente va a pesare sul buffer dei router precedenti nel percorso.

\subsubsection{Load shedding}
La soluzione finale quando i precedenti metodi non funzionano è il load shedding, il quale elimina il carico in più.
Ma quali pacchetti eliminare?\\
C'è la strategia wine, che elimina i pacchetti più recenti, o quella milk che elimina i più vecchi.
La scelta dipende dal contesto.
La cosa migliore è utilizzare un sistema di priorità dei pacchetti. 
Ovviamente tutti vorrebbero il massimo della priorità.
L'incentivo può essere economico (bassa priorità, basso costo), oppure un patto durante la creazione del circuito virtuale (il CV viene creato ma i pacchetti in eccesso prendono bassa priorità).

Una congestione va gestita appena rilevata, altrimenti blocca tutto.
Esiste l'algoritmo RED (Random Early Detection) che scarta i pacchetti prima che tutto il buffer sia pieno.
Questa tecnica è utilizzabile solo nelle reti cablate, dove il problema principale è l'esaurirsi del buffer e non gli errori di trasmissione.
Quindi conviene scartare prima che vadano persi, in questo modo la sorgente non riceve l'ack e rispedisce il pacchetto.
Per fare ciò si base sulla lunghezza media delle code; se supera la lunghezza di guardia, inizia la manovra correttiva.
In particolare, la sorgente sa che se sono persi i pacchetti è a causa della congestione, quindi oltre a tornare il pacchetto rallenta la velocità di trasmissione.

\subsubsection{Controllo del jitter}
In alcune trasmissioni la costanza del tempo di transito è fondamentale.
La variazione è detta jitter.
L'intervallo di tempo del jitter deve essere minimo.
Un router può controllare e limitare il jitter confrontando i tempi di arrivo di un pacchetto;
se è in anticipo, l'ho fa aspettare, se in ritardo, cerca di trasmetterlo subito.

\subsection{Qualità del servizio} % 5.4

\subsubsection{Requisiti}
Un flusso di pacchetti da sorgente a destinazione è chiamato flusso.
Le esigenze di un flusso hanno quattro parametri, che determinano la Qualità del Serzizio (QoS): affidabilità, ritardo, jitter e banda.

\subsubsection{Tecniche per una buona qualità}

\paragraph{Sovradimensionamento}
Una possibilità è quella di garantire molto spazio di buffer e banda al router per facilitare il transitare dei pacchetti.
Il problema ovviamente è il costo elevato. 

\paragraph{Utilizzo dei buffer}
Si può utilizzare un buffer nel dispositivo ricevente dove memorizzare il flusso prima di essere consegnati.
Permette di eliminare il jitter, aumentando però il ritardo.
La tecnica è utile per i servizi video/musicali, es. YouTube.

\paragraph{Traffic shaping}
La tecnica traffic shaping serve a rendere più uniforme il traffico lato server, regolando la velocità media della trasmissione dei dati.
Quando viene impostata una connessione, viene definito il modello di traffico tramite un accordo tra utente e sottorete: service level agreement.
Se l'utente rispetta l'accordo, l'operatore telefonico manderà i dati in modo veloce.
Il traffic policing è il controllo sul traffico, per vedere se sta venendo rispettato l'accordo.

\paragraph{Leaky bucket}
Questo algoritmo si basa sull'idea di un secchio con un buco sul fondo;
non importa con che velocità vengono trasmessi i pacchetti, esiste una coda finita nella quale vengono messi in attesa i pacchetti, i quali vengono processati a velocità costante (1 per ciclo di clock	), regolando così il flusso prima di essere immessi nella rete.
Se arrivano la coda è piena, i pacchetti successivi vengono scartati.
Funziona sia lato hw, sia sw; di per sè è facile da implementare.
Se i pacchetti hanno dimensione variabile, conviene usare come unità di misura processata un determinato blocco di bit, raggruppando così più pacchetti.

\paragraph{Token bucket}
Il token bucket cerca di risolvere una limitazione dell'algoritmo precedente, nel quale la velocità è fissa.
Vengono aggiunti dei token generati ad ogni clock.
Un pacchetto può venire trasmesso se ha un token a disposizione da distruggere.
Questo permette di avere dei burst di dati trasmessi, con un massimo uguale a \textit{n}, che equivale alla capacità del "secchio".
In questo modo, gli host inattivi accumulano token per trasmissioni successive.
Inoltre token bucket spreca i token, ma non i pacchetti: se non ci sono token, si ferma.

Esiste una variante che conta i byte per token invece che i pacchetti.

Il problema è la capacità di inviare un grande burst di dati; va quindi limitato adeguatamente il numero di token.

\subsection{Collegamento tra reti} % 5.5
Le reti collegate tra loro sono di tipo diverso e usano protocolli diversi per strato.
Probabilmente non verrà mai unificato in un unico tipo.

\subsubsection{Differenze tra le reti}
Le reti possono differire sotto vari aspetti, ma sono le differenze nello strato network che rendono difficile l'attraversamento di reti diverse.
Alcune differenze che possono creare problemi:
passare da una rete orientata alla connessione ad una senza connessione,
l'utilizzo di protocolli diversi, la diversa dimensione dei pacchetti, la diversa qualità del servizio, 
il controlle degli errori, del flusso e delle congestioni.

\subsubsection{Connessione tra le reti}
Nello strato network due reti sono collegate tramite router.
A volte il router può gestire protocolli diversi e in questo caso viene detto router multiprotocollo.

Confrontando un collegamento a livello data link e uno a livello network, ci sono diverse differenze:
nel datalink, viene utilizzato lo switch, nel quale viene guardato l'indirizzo MAC del frame per trovare la destinazione;
nel network, la comunicazione avviene tramite due router collegati punto a punto, uno dei quali estrae il pacchetto dal frame, esamina l'indirizzo e lo confronta con la tabella di routing e mandato nel secondo router, dove viene messo in un frame per la seconda rete.

La differenza principale tra i due è che nello strato network gli switch ignorano il protocollo dello strato network utilizzato, mentre i router si.

\subsubsection{Circuiti virtuali concatenati}
Come già detto, esistono reti orientate alla connessione, che usano i circuiti virtuali, e quelle senza connessione, che usano i datagrammi.
Nella rete a circuiti virtuali concatenati, quando è connessa con altre reti lontane viene costruito un circuito virtuale tra i router delle due reti con un gateway (router multiprotocollo) di mezzo, che fa da intermediario.
Il processo si ripete se deve attraversare più sottoreti per giungere a destinazione.
Quando un pacchetto giunge ad un gateway, viene inoltrato e convertito al formato usato dalla rete successiva.
La sequenza di circuiti virtuali è impostata dalla sorgente e tutti i pacchetti devono attraversare la stessa sequenza di gateway.
Questa tecnica va bene quando si hanno circa le stesse proprietà di qualità del servizio.

\subsubsection{Collegamento tra reti senza connessione}
In questo caso ci sono sempre i gateway tra le sottoreti, ma ogni pacchetto può seguire una strada diversa in base al traffico.
Questo metodo può ottenere più banda ma non ha garanzia di ordine d'arrivo dei pacchetti.

Notare che le conversioni del formato di pacchetto nel router multiprotocollo è fattibile se sono due formati simili;
se differiscono troppo.
Un secondo problema sono gli indirizzi di destinazione, che possono essere di formato diverso e quindi non comprensibili.

\subsubsection{Routing in una internetwork}
Il routing in una internetwork si complica rispetto la singola sottorete.
Di fatto all'interno di una rete viene usato un protocollo di gateway interno, mentre tra le reti un protocollo di gateway esterno.
Infatti ogni rete è indipendente dalle altre e spesso viene denominata Autonomous System (AS).
Nel concreto un pacchetto giunge al router multiprotocollo, il quale poi decide verso quale rete instradarlo.

\subsection{Lo strato network in internet} % 5.6
La rete di internet è in insieme di sottoreti o AS connessi tra loro.
La "colla" di fondo è il protocollo dello strato network chiamato IP (Internet Protocol).

\subsubsection{Il protocollo IPv4}
Un datagramma IP ha il seguente formato:
\begin{itemize}
\item intestazione, di 20 byte fissi + parte opzionale variabile; i campi sono:
\begin{itemize}
\item versione, il quale indica la versione del protocollo usato, permettendo di sviluppare e passare a nuove versioni (es. IPv4 a IPv6)
\item IHL indica la lunghezza dell'intestazione espressa in numero di parole da 32 bit
\item type of service distingue le varie classi di servizio e i parametri del QoS
\item total lenght indica la lunghezza totale del datagramma (intestazione + testo)
\item identification indica a quale datagramma appartiene il frammento arrivato
\item DF (don't fragment) indica di non frammentare il datagramma perchè la destinazione non riuscirebbe a ricostruirlo
\item MF (more fragments) indica che è un frammento e deve ancora arrivare l'ultimo pezzo, che avrà il bit del campo a zero
\item Fragment offset indica la posizione del frammento nel datagramma
\item time to live limita la vita di un pacchetto, in modo da non lasciare un pacchetto in giro per la rete per troppo tempo
\item protocol indica il processo di trasporto, come TCP o UDP
\item header checksum verifica l'intestazione e viene ricalcolato ad ogni salto
\item source e destination address che indicano l'indirizzo sorgente e di destinazione
\item option che indica diverse opzioni aggiuntive
\end{itemize}
\item testo
\end{itemize}

\subsubsection{Gli indirizzi IP}
Ogni host e router ha un indirizzo IP univoco che indica l'indirizzo di rete e il numero di host. 
Gli indirizzi sono lunghi 32 bit e si riferiscono ad una scheda di rete più che al singolo host, ovvero se un host ha più schede necessità di più indirizzi.

Per molti decenni gli indirizzi IP erano divisi in 5 categorie dette classi, dove ogni classe rappresentava un formato diverso;
ora non è più usato questo indirizzamento per classi, ma viene ancora usato nella letteratura informatica, quindi in sintesi:
\begin{itemize}
\item A, B, C - rete + host in diverse lunghezze 
\item D - Per indirizzi multicast
\item E - Riservata per usi futuri
\end{itemize}
Le classi supportano rispettivamente un massimo di:
\begin{itemize}
\item 128 reti con 16 milioni di host (A)
\item 16.384 reti e 64.000 host (B)
\item 2 milioni di reti e 256 host (C)
\end{itemize}

Gli indirizzi usano la notazione decimale a punti: 0.0.0.0 il minimo, 255.255.255.255 il massimo.
Già da questo si può notare che il numero è limitato, il che crea problemi in un universo di reti sempre più in espansione.
Esistono dei valori che hanno un significato specifico, come lo 0 che indica la rete o l'host corrente, o il 127 che è usato in loopback, per simulare pacchetti in arrivo e che non escono dalla rete.

\paragraph{Sottoreti}
Tutti gli host di una rete devono avere lo stesso numero di rete, ma è un problema quando le reti crescono.
La rete viene quindi divisa in più sottoreti e viene vista esternamente come una unica.

Se venisse utilizzato l'indirizzamento a classi, creare e gestire la tabella di routing del router centrale che collega le sottoreti sarebbe oneroso.
Quindi viene modificato l'indirizzo di classe B togliendo alcuni bit dal numero di host per usarli come numero di sottorete. 
Per usare le sottoreti il router principale ha bisogno della maschera di sottorete, che indica il punto di demarcazione tra numero di rete, sottorete e host. 
Questa divisione è solo interna alla rete e non viene vista al di fuori, quindi può essere gestita autonomamente.
La notazione della maschera può essere decimale a punti o con \texttt{/numero} che indica la lunghezza della maschera di sottorete.

Come vengono elaborati i pacchetti IP dai router? 
Ogni router ha una tabella con gli indirizzi IP \textit{rete-0} e \textit{questa rete-host};
indicano come raggiungere reti lontane e come raggiungere quelle locali.
Quando un pacchetto è ricevuto, cerca la destinazione nella tabella: 
se è una rete distante viene inoltrato, se è locale viene inviato all'host.
Se non viene trovato l'indirizzo, viene inoltrato ad un router con tabelle più dettagliate.
Questa tecnica riduce di molto la grandezza delle tabelle.

Utilizzando la divisione in sottoreti, chiaramente cambiano le tabelle di routing, dove le voci diventano
\textit{questa rete-sottorete-0} e \textit{questa rete-questa sottorete-host}.
Quindi un router su una sottorete k sa come raggiungere le altre sottoreti e come raggiungere gli host della sottorete k.

\paragraph{CIDR}
Un problema di IP è che si stanno esaurendo gli indirizzi.
Il principale colpevole è l'indirizzo di classe B che è un buon compromesso tra A e C, quindi venne molto usato, però in molti casi rimane troppo grande e risulta uno spreco e la C sarebbe bastata, ma questo tipo avrebbe appesantito di molto le tabelle di routing.

La soluzione usata al momento è CIDR (Classless Interdomain Routing), il quale assegna gli indirizzi IP rimanenti in blocchi, ignorando le classi.
Questo però complica l'inoltro:
prima l'indirizzo IP estratto da un pacchetto veniva letto per capire di che classe era, poi attraverso una diramazione a 16 percorsi in base alla classe veniva letto il numero di rete e cercata la voce nella tabella per inoltrare il pacchetto nella linea giusta;
nel CIDR invece ogni voce della tabella ha una maschera di 32 bit, quindi una linea è composta da \textit{indirizzo IP-maschera di sottorete-linea di output}.
Quando giunge un pacchetto, viene estratto l'indirizzo IP di destinazione, viene mascherato e confrontato con la tabella di routing;
viene scelta la linea della tabella con il valore della maschera (se ci sono più voci uguali ma di lunghezza diversa, viene presa la maschera più lunga) e viene inoltrato nella linea indicata.

\paragraph{NAT}
Anche il NAT esiste per il problema riguardante il numero di indirizzi, che è limitato ma non sufficiente a coprire il numero crescente di dispositivi.
Un ISP ha un numero limitato di host a cui può offrire servizio e utenti commerciali con molti computer sempre accesi "consumano" la disponibilità.
Inoltre la diffusione di ADSL pone degli IP fissi per quel determinato cliente, che può essere potenzialmente sempre connesso (e potenzialmente anche con più dispositivi, tutti con IP diverso), limitando ulteriormente la disponibilità di indirizzi IP.
La soluzione a lungo termine è il passaggio alla versione successiva di IP, che usa indirizzi a 128 bit, ma è un processo molto lungo e lento.

Invece la soluzione attualmente adottata è il NAT (Network Address Translation), nel quale viene associato un solo indirizzo IP (o un numero limitato) ad ogni rete locale;
all'interno della rete ogni dispositivo ha un suo indirizzo IP, ma quando un pacchetto deve uscire, viene tradotto dal dispositivo NAT nell'indirizzo IP assegnatogli.
Spesso il dispositivo NAT è accoppiato al firewall.

Il problema principale del metodo è l'arrivo di un pacchetto.
Come capire a quale indirizzo interno deve arrivare?
Per fare ciò viene usata l'intestazione dei carichi utili TCP e UDP, in quanto modificare l'indirizzo IP non era fattibile.
In particolare viene usato il campo porta sorgente/destinazione, sempre presenti nei carichi utili.
Quando un pacchetto è in uscita, l'indirizzo sorgente è tradotto nell'indirizzo IP assegnato, mentre il campo porta sorgente ottiene un indice che punta alla riga della tabella corrispondente del NAT, che contiene le coppie \textit{indirizzo IP-porta}.
Quando arriva il pacchetto, viene estratto l'indice nel campo porta sorgente, cercata la voce nella tabella e tradotto l'indirizzo IP nell'indirizzo locale.

Obiezioni contro NAT:
\begin{itemize}
\item viola il modello gerarchico di IP (un dispositivo, un indirizzo IP)
\item trasforma la rete internet da senza connessione a orientata alla connessione, perchè deve mantenere le informazioni
\item viola la stratificazione dei protocolli, che li rende indipendenti tra loro
\item i processi possono usare qualcosa che non sia TCP o UDP
\item la lunghezza del campo porta sorgente limita il numero di macchine associate ad un indirizzo IP
\end{itemize}

\subsubsection{Protocolli di controllo internet}
Esistono diversi protocolli di controllo Internet

\paragraph{ICMP}
I router monitorano Internet e con ICMP (Internet Control Message Protocol) notificano se succede qualcosa di anomalo.
Questo protocollo definisce una serie di messaggi di controllo incapsulati in pacchetti IP.
Alcuni esempi (non approfondito):
\begin{itemize}
\item Destination unreachable
\item Time Exceeded
\item Parameter problem
\item Source quench
\item Redirect
\item ECHO
\end{itemize}

\paragraph{ARP}
Ogni scheda di rete ha un proprio indirizzo di 48 bit univoco; le schede inviano i frame in base a questi indirizzi e non sanno niente degli indirizzi IP.
Come avviene l'associazione indirizzi strato data link e indirizzi IP?
L'uso di un file di configurazione è impensabile, quindi si usa il protocollo ARP (Address Resolution Protocol), nel quale un host manda un pacchetto broadcast è chiede chi è il proprietario di un certo indirizzo IP, recuperato per esempio tramite il DNS.
Il proprietario dell'indirizzo risponde, riuscendo così a fare l'associazione dell'indirizzo IP al relativo indirizzo Ethernet.

\paragraph{DHCP}
Esiste il problema inverso, ovvero associare un indirizzo Ethernet ad un indirizzo IP.
Per fare ciò si usa il protocollo DHCP (Dynamic Host Configuration Protocol), il quale si basa su un server che assegna gli indirizzi IP.
Ogni LAN deve avere un agente di inoltre al server DHCP, il quale gestisce le trasmissioni DHCP e comunica con il server.

\subsubsection{OSPF}
Come già detto, internet è un insieme di reti dove il routing interno è gestito indipendentemente, e quello esterno è univoco.
Conviene però cercare di porre uno standard, e qui si presenta OSPF (Open shortest path first), che sostituisce gli algoritmi basati sul vettore delle distanze e sullo stato dei collegamenti.

I principi su cui è stato creato l'algoritmo sono:
\begin{itemize}
\item l'algoritmo deve esere aperto
\item deve supportare diverse metriche (distanza, ritardo ecc)
\item doveva essere dinamico ed adattarsi alla situazione
\item doveva supportare il routing in base al tipo di servizio usando il relativo campo IP \textit{type of service}, e questo era una novità
\item doveva bilanciare il carico, suddividendolo tra le linee
\item doveva supportare i sistemi gerarchici
\item doveva presentare un minimo di sicurezza
\item doveva provvedere a gestire i router collegati tramite tunnel
\end{itemize}

Sono supportate tre tipi di connessioni e reti:
\begin{itemize}
\item punto a punto tra due router
\item reti multiaccesso con trasmissione broadcast
\item reti multiaccesso senza broadcast
\end{itemize}

L'intero insieme di reti, router, linee è rappresentato da un grafico orientato, con i costi negli archi.
Il percorso più breve viene calcolato in base al peso negli archi.

Se una AS è troppo grande viene suddivisa in aree (delle "sottoreti") e ogni AS ha un'area dorsale a cui sono collegate tutte le aree.
I router che collegano due aree devono avere la tabella che mappa entrambe le aree.

Tre tipi di percorsi: interni all'area, conosce già il percorso più breve;
tra aree, dove sorgente -> dorsale -> area destinazione -> destinazione;


\subsubsection{BGP}
Tra AS invece, si utilizza il protocollo BGP (Border gateway protocol).
Nel caso delle AS c'è la politica di mezzo, quindi serve una sorta di "dogana".
I criteri sono configurati manualmente.
[...]

\subsubsection{Internet multicasting}
Tramite gli indirizzi di classe D, IP supporta il multicasting, ovvero la trasmissione di un pacchetto a più riceventi.
I 28 bit sono usati per identificare i gruppi;
quando vengono trasmessi i pacchetti, non c'è nessuna garanzia di consegna.

Gli indirizzi di gruppo possono essere temporanei (vanno creati) o permanenti (sempre presenti e impostati).

Il routing viene effettuato usando strutture spanning tree.


\subsubsection{IPv6}
IPv6 è la versione successiva a IPv4 (esiste anche il 5, ma non viene usato).
Questa versione è una soluzione a lungo termine (perché tutta la rete va adattata) al problema del numero di indirizzi (e non solo).

Gli obiettivi di IPv6:
\begin{itemize}
\item supportare più host
\item ridurre la dimensione delle tabelle di routing
\item semplificare il protocollo
\item renderlo più sicuro
\item supportare più tipi di servizio
\item aiutare il multicast
\item dare la possibilità ad un host di spostarsi senza cambiare indirizzo
\item permettere un'evoluzione futura
\item compatibilità tra protocolli vecchi e nuovi
\end{itemize}
IPv6 non è compatibile con il 4, ma lo è con i protocolli ausiliari, come TCP, UDP, DNS, ecc.

IPv6 ha indirizzi più lunghi, ha un'intestazione più semplice (7 campi), un miglior supporto per le opzioni, più attento alla QoS e più sicuro (anche se in seguito la sicurezza di IPv4 è stata migliorata e non è così grande la differenza).

\paragraph{L'intestazione principale}
L'intestazione da 7 campi.
Version indica la versione e ha sempre 6.
Traffic class è usato per i pacchetti che trasportano dati in tempo reale, ma è ancora in perfezionamento il suo uso.
Flow label, sperimentale, pone dei requisiti alla connessione tra sorgente e destinazione.
Payload lenght indica la lunghezza del carico, senza contare l'intestazione.
Next header contiene i campi opzionali usati da IPv4 per mantenere la compatibilità.
Hop limit indica dopo quanti salti un pacchetto viene scartato.
Infine i due campi indirizzo della sorgente e della destinazione.
la notazione dell'indirizzo cambia: 8 gruppi da 4 cifre esadecimali.

\paragraph{Intestazione estesa}
L'intestazione estesa contiene dei campi che potrebbero servire in alcune situazioni (che erano in IPv4), per essere compatibile con i protocolli che usavano quei campi.

\paragraph{Controversie}
[...]

\newpage
\section{Capitolo 6 - Strato trasporto}
Il livello di trasporto si basa sul livello di rete per fornire il trasporto dei dati tra due macchine secondo il livello di affidabilità desiderato e indipendentemente dalle reti fisiche. 
L'obiettivo finale è fornire un servizio efficiente, efficace ed affidabile ai suoi utenti.
Per fare ciò usa i servizi forniti dallo strato di rete. L'hardware/software usato per svolgere il lavoro è detto \textbf{entità di trasporto}.

Per molti aspetti lo strato trasporto può sembrare simile a quello network, ma il fatto è che questo strato è essenziale per garantire affidabilità che lo strato network, basato sulla rete, non può dare. 

Grazie alle primitive fornite dallo strato, i programmatori possono scrivere codice senza preoccuparsi della sottorete.

In seguito verrà usato TPDU (transport protocol data unit) per indicare i messaggi inviati da un'entità di trasporto ad un'altra. 
Quindi, riprendendo gli altri "contenitori" degli altri strati, avremo TPDU dentro un pacchetto, il quale è dentro un frame
(trasporto -> network -> datalink).
Ogni carico utile di ogni livello ha la propria intestazione. 

\subsection{Richiesta connessione e protocollo Three-way Handshake}

Il Three-way handshake è un protocollo che entra in gioco quando si deve stabilire una connessione.
Infatti stabilire una connessione è un'operazione tutt'altro che facile.
Al normale "percorso" CONNECTION REQUEST - CONNECTION ACCEPTED, la rete può perdere, ritardare, corrompere o duplicare i pacchetti, creando diversi problemi.
In particolare, l'esistenza dei duplicati ritardati è il nodo centrale del problema e può essere affrontato in diversi modi.
Una possibilità è l'utilizzo di indirizzi di trasporto monouso oppure si può assegnare ad ogni connessione un identificatore inserito in ogni TDPU.
Entrambe le possibilità hanno diversi problemi.

La soluzione è cercare di distruggere i pacchetti obsoleti rimasti in circolo, impostando una durata massima di vita del pacchetto.
Il valore massimo di durata può essere prefissato utilizzando una o più tecniche tra le seguenti:
\begin{itemize}
\item progettazione di sottoreti limitate che impediscono ai pacchetti di essere ripetuti ciclicamente
\item inserimento di un contatore di salti in ogni pacchetto che viene decrementato ad ogni salto e se raggiunge lo zero viene scartato
\item applicazione di un contrassegno temporale in ogni pacchetto; verrà scartato se diventa troppo vecchio secondo un tempo stabilito
\end{itemize}
Oltre al pacchetto, anche i suoi acknoledgment devono essere distrutti.
Consideriamo quindi un valore T che indica il tempo dopo il quale si può essere certi che tutte le tracce del pacchetto sono scomparse (sia pacchetto che acknoledgement).
La sorgente etichetta i segmenti con numeri di sequenza che non verranno riutilizzati per T secondi.
In questo modo ci sarà un unico pacchetto con quel numero di sequenza in sospeso.

Nel caso di malfunzionamento, è possibile far rimanere le entità inattive per T secondi in modo da far scadere gli eventuali segmenti vecchi, così da essere sicuri di non usare numeri di sequenza già utilizzati.
Tuttavia su reti complesse non conveniva, quindi fu proposto un orologio per ogni host implementato come contatore binario che abbia tanti bit quanti quelli del numero di sequenza.
Quando viene instaurata la connessione, i k bit di ordine più basso dell'orologio sono usati come primo numero di sequenza, quindi ogni connessione inizia con numeri diversi.
Una volta stabilito il primo numero di sequenza, si può utilizzare un qualsiasi protocollo sliding window.

Il metodo dell'orologio risolve il problema dei segmenti duplicati, ma non c'è modo si ricordare i numeri tra una connessione e l'altra, quindi non sappiamo se un segmento di richiesta di connessione sia duplicato o meno.
La soluzione è il three-way handshake che richiede la verifica reciproca da parte dei peer utilizzando tre messaggi.
Il primo host manda la richiesta di connessione con un numero di sequenza x; l'host 2 risponde con un ack che conferma x e contiene il suo numero di sequenza iniziale. L'host 1 allora invia il primo segmento dati e conferma a sua volta la scelta del numero di sequenza iniziale dell'host 2.
Nel caso di richiesta di connessione duplicata, l'host 2 reagisce normalmente, ma al terzo messaggio l'host 1 rifiuta il tentativo di connessione.
Se invece si hanno sia la richiesta che l'ack ritardati, l'host 2 reagisce normalmente restituendo il numero y, successivamente l'host riceve il secondo segmento ritardato che ha però un ack su un numero diverso da y, per cui capisce che è un duplicato.

\subsection{Rilascio della connessione}
Il rilascio della connessione è più semplice e può essere simmetrico o asimmetrico.
In quello asimmetrico, una delle due parti interrompe la connessione; è improvviso e può far perdere di dati.
Il rilascio simmetrico tratta la connessione come se fosse composta da due connessioni separate unidirezionali, quindi le rilascia separatamente.
In questo caso, anche se una delle due parti ha inviato il pacchetto DISCONNECT, può continuare a ricevere dati.

Anche per il rilascio conviene usare il three-way handshake, anche se in questo caso non è infallibile.
Ogni messaggio passa la richiesta di disconnessione e il ricevente deve confermare con una richiesta di rilascio; infine il primo risponde con l'ack.
in questo caso, ogni messaggio ha un timer per triggerare la rispedizione nel caso qualcosa vada storto.

\subsection{Introduzione all'UDP}
UDP (User Datagram Protocol) è un protocollo di trasporto senza connessione, permette cioè di inviare datagrammi IP senza dover stabilire una connessione.
UDP trasmette segmenti con un'intestazione di 8 byte seguita dal carico utile.
L'intestazione contiene le porte dell'origine e della destinazione.
Il campo UDP lenght include l'intestazione e i dati, mentre UDP checksum contiene il checksum.

UDP non si occupa del controllo del flusso, degli errori o della ritrasmissione.

Questo protocollo è particolarmente utile per il multimediale (streaming) e per le situazioni client/server,
dove è richiesta una risposta breve ad una breve richiesta al server.
Se non viene ricevuta la risposta, allora va in timeout e riprova.
Questo tipo di utilizzo viene effettuato dal DNS.

\subsection{TCP}

\subsubsection{Introduzione}
TCP (Transport Control Protocol) è un protocollo di trasporto creato per fornire affidabilità per un internetwork inaffidabile.
Un internetwork ha diversi parametri che possono variare e TCP cerca di adattarsi dinamicamente.
Un'entità di trasporto TCp riceve i dati dell'utente, li suddivide in pezzi di dimensione massima uguale a 64KB e invia ogni pezzo in un datagramma 
IP autonomo.
Nel ricevente i datagrammi vengono usati per ricostruire il flusso di byte originali. 
TCP ha il compito di segnalare gli errori, richiedere la ritrasmissione e riordinare i datagrammi se non sono nell'ordine corretto.

\subsubsection{Modello di servizio di TCP}
Il servizio TCP è ottenuto creando punti finali da parte del mittente e ricevente, ovvero i socket.
Ogni socket ha un indirizzo composto dall'indirizzo IP dell'host e da un numero di 16 bit locale all'host, che indica la porta.
Per avere un servizio è necessario stabilire una connessione tra i due socket.

Un socket supporta più connessioni alla volta e una connessione è identificata dalla coppia dei due socket utilizzati.

I numeri di porta inferiori a 1024 sono le well-known ports, porte speciali dedicate ai servizi standard.

Tutte le connessioni TCP sono full-duplex punto a punto. Non supporta né il broadcsting, né il multicasting.

Una connessione TCP è un flusso di byte e non di messaggi, quindi un messaggio può essere trasmesso spezzato in blocchi di byte più piccoli o con altri messaggi. 

TCP ha la capacità di decidere se inviare subito i dati ricevuti da un'applicazione o se salvarli in un buffer per raggrupparli e inviarli insieme ad altri.
L'applicazione può avere però richieste particolare e chiedere che sia spedito subito tramite il flag PUSH.

\subsubsection{Protocollo TCP}
Una funzionalità importante del protocollo è il fatto che ogni byte ha un proprio numero di sequenza a 32bit.

I dati vengono scambiati sotto forma di segmenti.
Un segmento consiste di un'intestazione di 20 byte seguita da 0 (ack) o più byte di dati.
Sta a TCP decidere la grandezza massima dei segmenti e può decidere se dividere o combinare i messaggi.
Il limite superiore però è di 65615 byte, che deve starci nel carico utile di IP.
Ogni rete ha un MTU (Maximum Transfert Unit) e il segmento deve stare sotto la soglia.
Di solito è di 1500 byte.

Le entità TCP usano il protocollo sliding window.

\subsubsection{Intestazione TCP}
L'intestazione di un segmento è di 20 byte.
Due campi sono dedicati alla porta d'origine e a quella di destinazione.
I campi \textit{numero di sequenza} e \textit{numero di acknowledgement} svolgono le solite funzioni.
Il campo \textit{lunghezza dell'header TCP} indica quante parole di 32 bit sono contenute nell'intestazione; 
è necessario perchè il campo options ha una lunghezza variabile.
Seguono 6 bit inutilizzati e 6 bit per 6 flag, come URG (puntatore urgente), ACK (acknowledgement è valido), PSH (push), RST (reimposta la connessione), SYN (stabilisce una connessione), FIN (rilascia la connessione).
Il campo successivo \textit{window size} indica la dimensione della finestra per lo sliding window e in particolare quanti byte possono essere inviati a partire da quello che ha ricevuto l'acknowledgement.
Seguono il campo checksum e l'urgent pointer.
Una volta finita l'intestazione, sono presenti il campo \textit{options} per opzioni aggiuntive e i dati effettivi.

\subsubsection{Connessione TCP}
Le connessioni vengono stabilite secondo il three-way handshake.
Un host richiede la connessione inviando un segmento con bit SYN a 1 e ACK a 0 e attende la risposta.
L'host ricevente controlla se alla porta indicata nel frame è in ascolto; in caso negativo risponde rifiutando la connessione, 
altrimenti risponde con un segmento di acknoledge per dare il via libera al primo host, che manda il segmento di dati.

Se due host tentano una connessione sugli stessi socket, solo una delle due andrà a buon fine in quanto i socket sono identificativi della connessione.

\subsubsection{Rilascio connessione TCP}
Il rilascio avviene in maniera simmetrica, dove ogni parte può inviare un segmento con il bit FIN impostato per dire che ha finito di trasmettere e può essere chiusa la connessione.
Nel senso opposto può continuare la trasmissione finché non termina e viene rilasciata del tutto la connessione.

Per rilasciare la connessione servirebbero 4 segmenti (2 FIN e 2 ACK), ma è possibile raggruppare il primo ACK con il secondo FIN in modo da averne solo tre.

Se ad un FIN non arriva la risposta, un timer rilascia la connessione nella direzione del FIN inviato.

\newpage
\section{Capitolo 7 - Strato applicazione}

Lo strato applicazione e dove si trovano effettivamente le applicazioni.
Ci sono comunque dei protocolli di supporto per permettere alle applicazioni di funzionare.
Uno di questi è il DNS.

\subsection{DNS}

Il DNS (Domain name system) è un protocollo per la gestione dei nomi.
I siti web e le altre risorse possono essere accedute direttamente tramite l'indirizzo IP,
ma non risulta molto user-friendly in quanto per l'utente è difficile memorizzare l'indirizzo della risorsa richiesta.
Il DNS serve per tradurre gli indirizzi IP in nomi comprensibili e viceversa.
Infatti, se per l'utente il nome è più comprensibile, il network comprende solo l'indirizzo IP, per cui è richiesta la traduzione inversa da nome a indirizzo.
Un altro problema che ha portato alla creazione del DNS era la necessità di un sistema centralizzato, che permettesse di gestire i nomi senza avere duplicati.

Il DNS quindi è un sistema centralizzato con un database distribuito dove viene implementato lo schema gerarchico dei nomi basato su dominio. 
Viene utilizzato principalmente per mappare indirizzi IP e nomi del relativo host.
Per mappare un nome con il suo indirizzo IP, viene chiamata una procedura detta \texttt{resolver} con il nome come parametro.
Questa invia la query al server DNS e viene ritornato l'indirizzo IP; sia la query, sia la risposta sono spediti come pacchetti UPD.

La gerarchia dei nomi è divisa in diversi livelli. 
Internet è diviso in più di 250 domini di primo livello contenenti i vari host. 
Ogni dominio è partizionato in sottodomini, anch'essi partizionati ecc.
Il primo livello è distinto tra generici (com, org, edu, ecc) e nazioni (it, uk, jp, ecc).
Il secondo livello spesso indica l'azienda.
Il nome di un dominio può essere assoluto o relativo.
Se termina con il punto è assoluto; se è relativo, il reale significato dipende dal contesto.
I nomi sono case-insensitive e di lunghezza massima di 255 caratteri.

Ogni dominio può essere associato ad un insieme di record delle risorse.
Quando il DNS riceve un nome, restituisce i record associati, tra cui quello che indica l'indirizzo IP.
Un record è identificato dalla quintupla:\\
\texttt{Domain\_name Time\_to\_live Class Type Value}

Il Type A è il più importante in quanto è il record che contiene l'indirizzo IP. Questo record può non essere univoco.

Il server del DNS non è unico, altrimenti verrebbe sovraccaricato. Quindi i nomi sono divisi in zone, che risiedono su server diversi. 

\newpage
\section{Capitolo 8 - Sicurezza}

\subsection{Crittografia} % 8.1
Quando si parla di crittografia vanno distinti due termini: cifrario, ovvero la trasformazione carattere per carattere del messaggio, e codice, che rimpiazza ogni parola con un'altra parola (non più usati).

\subsubsection{Introduzione alla crittografia}
Un messaggio da cifrare è detto testo in chiaro ed è trasformato in testo cifrato da una funzione secondo una chiave di cifratura.
Decifrare è l'operazione legittima di lettura dei dati, traducendoli nel messaggio originale, mentre decriptare è l'attività di decifrazione da parte dell'intruso.

Secondo il principio di Kerckhoff, tutti gli algoritmi di cifratura devono essere pubblici e solo le chiavi sono segrete.
Infatti di solito cercare di tenere nascosto l'algoritmo non è produttivo, mentre averlo pubblico può portare a collaborazioni e miglioramenti.

Dato che la segretezza sta nella chiave, la sua lunghezza è la parte fondamentale.
Più è lunga, più è difficile da decriptare.

\subsubsection{Cifrari a sostituzione}
Il cifrario a sostituzione sostituisce una o più lettere con una o più lettere.
Uno dei cifrari più antichi è quello di Cesare, che effettuava uno slittamento dell'alfabeto di 3 lettere: A diventa D, B diventa E, ecc.
In generale, si può usare questo metodo usando una chiave \textit{k} per spostare di \textit{k} lettere l'alfabeto.
Questa cifratura è detta sostituzione monoalfabetica.

Il passaggio successivo è stato far cambiare la corrispondenza lettera in chiaro e lettera cifrata in maniera più casuale.
La chiave è la stringa che corrisponde all'intero alfabeto.

Anche se questo sistema sembra sicuro per il gran numero di combinazioni, 
più testo cifrato si ha a disposizione e più facile è decriptare il testo grazie alle proprietà statistiche dei linguaggi.
Basterà usare diagrammi o trigrammi più comuni (coppie o triple di lettere), o cercare di individuare una parola che è probabile che sia presente.

\subsubsection{Blocchi monouso}
Il blocco monouso (one time pad) permette di costruire un cifrario imbattibile: 
\begin{enumerate}
\item Si genera una chiave formata da una stringa di bit casuali
\item Si converte il testo in chiaro in una stringa di bit, per esempio secondo il codice ASCII
\item Si calcola lo XOR delle due stringhe per ottenere il testo cifrato
\end{enumerate}
Il risultato non è decriptabile perché in un testo grande ogni lettera comparirà con la stessa frequenza ed è immune a tutti gli attacchi.

Nonostante sia ottimo a livello teorico, nella pratica ha diversi svantaggi.
La chiave per esempio non è possibile impararla a memoria, quindi entrambi i comunicanti devono averne una copia.
Inoltre la lunghezza della chiave limita la quantità di dati che può essere trasmessa.
Infine, se venisse persa la sincronizzazione tra i due comunicanti a causa di caratteri persi o aggiunti per errore, il resto dei dati sarebbe illeggibile.

\subsubsection{Due principi crittografici fondamentali}
I due principi crittografici fondamentali sono la ridondanza e l'attualità.

Il primo principio afferma che tutti i messaggi cifrati devono contenere una qualche forma di ridondanza, cioè un'informazione non necessaria al messaggio.
Questo per evitare che gli intrusi possano creare dati casuali con il rischio che vengano interpretati come validi.
La ridondanza però renderebbe più facile decriptare: quindi difende contro gli intrusi attivi, ma meno da quelli passivi. 

Il secondo principio afferma che è necessario avere la possibilità di verificare che ogni messaggio ricevuto sia attuale, recente, in modo da poter prevenire attacchi di tipo ripetizione, dove un intruso manda messaggi vecchi spacciandoli per nuovi.

\subsection{Algoritmi a chiave simmetrica} % 8.2
Questi algoritmi usano la stessa chiave per cifrare e decifrare.
Gli algoritmi possono essere implementati in software o hardware (es: P-box, scatola di permutazione con input a 8 bit, o S-box, scatola di sostituzione).

\subsubsection{DES singolo e triplo}
DES (Data Encryption Standard) cifra il testo in chiaro in blocchi di 64 bit che generano 64 bit di testo cifrato.
Utilizza una chiave a 56 bit e ha 19 stadi, dove nel primo viene fatta la trasposizione dei 64 bit di testo in chiaro,
indipendentemente dalla chiave, mentre nell'ultimo stadio avviene l'inverso del primo. 
Quelli intermedi hanno la stessa funzione, ma parametrizzati da diverse funzioni della chiave.

Ogni stadio intermedio prende due ingressi a 32 bit e produce due uscite a 32 bit:
quello di sinistra è la copia dell'originale di destra, mentre quello di destra è lo XOR tra i bit di sinistra, quelli risultanti da una funzione che ha 
come parametri i bit di destra e della chiave di stadio.

\`E possibile rafforzare il DES con la tecnica dello sbiancamento, la quale effettua lo XOR di ogni blocco dati del testo in chiaro con una chiave casuale a 64bit all'entrata e all'uscita del DES. 

Quando IBM si rese finalmente conto che la chiave di cifratura era troppo corta, decise di usare una cifratura tripla.
Vengono usate due chiavi e tre stadi: nel primo il testo in chiaro viene cifrato con DES usando la chiave $K_1$, nel secondo viene usato DES in modalità di decifrazione usando $K_2$, infine un'altra cifratura con chiave $K_1$.
Questa alternativa migliorata è detta \textbf{triplo DES}.\\
Domanda 1: perché solo 2 chiavi e non 3? Perché i 112 bit erano ritenuti sufficienti, mentre 168 avrebbe richiesto molto lavoro senza vantaggi effettivi.\\
Domanda 2: perché cifra-decripta-cifra e non cifra-cifra-cifra? Il motivo è per mantenere la compatibilità con i sistemi che usavano un solo DES, ponendo $K_1=K_2$.

\subsubsection{AES}
AES (Advanced Encryption Standard) è lo standard successivo a DES.
Questo algoritmo è un cifrario simmetrico a blocchi, con struttura interamente pubblica (DES non lo era), supporta chiavi fino a 256 bit, è implementabile sia via software, sia hardware e doveva essere pubblicato senza vincoli.

Questo algoritmo deriva da un concorso pubblico;
ci furono diverse proposte e alla fine quella definita vincitrice fu quella di Rijndael.

AES definisce che la dimensione del blocco doveva essere di 128 bit, mentre quella della chiave poteva essere di 128 o 256 bit.
Ci sono quindi le versioni 128/128 e 128/256, ma già la prima è sufficientemente sicura.

Come DES, AES usa sostituzioni e permutazioni in diversi round (10 a 128 bit) e a differenza di DES le operazioni coinvolgono byte interi.

AES opera utilizzando matrici di 4x4 byte chiamate stati (states).
Quando l'algoritmo ha blocchi di 128 bit in input, la matrice State ha 4 righe e 4 colonne;
se il numero di blocchi in input diventa di 32 bit più lungo, viene aggiunta una colonna allo State, e così via fino a 256 bit.
In pratica, si divide il numero di bit del blocco in input per 32 e il quoziente specifica il numero di colonne.

C'è un passaggio iniziale:\\
AddRoundKey – Ogni byte della tabella viene combinato con la chiave di sessione, la chiave di sessione viene calcolata dal gestore delle chiavi.
Successivamente per cifrare sono previsti diversi round o cicli di processamento: ogni round (fase) dell'AES (eccetto l'ultimo) consiste dei seguenti quattro passaggi:
\begin{itemize}
\item SubBytes – Sostituzione non lineare di tutti i byte che vengono rimpiazzati secondo una specifica tabella.
\item ShiftRows – Spostamento dei byte di un certo numero di posizioni dipendente dalla riga di appartenenza.
\item MixColumns – Combinazione dei byte con un'operazione lineare, i byte vengono trattati una colonna per volta.
\item AddRoundKey – Ogni byte della tabella viene combinato con la chiave di sessione, la chiave di sessione viene calcolata dal gestore delle chiavi.
\end{itemize}

Il numero di round o cicli di processamento/elaborazione crittografica dei quattro passaggi precedenti è 10 con l'ultimo round che salta il passaggio MixColumns.

La fase di decifratura non è identica a quella di cifratura dal momento che gli step sono eseguiti in ordine inverso.
Tuttavia, si può definire un cifrario inverso equivalente ai passi dell'algoritmo usato per la cifratura, usando la funzione inversa a ogni step e un different key schedule.
Funziona siccome il risultato non cambia quando si scambiano la fase di SubBytes con quella di ShiftRows, e quella di MixColumns con una fase aggiuntiva di AddRoundKey.

L'algoritmo è molto sicuro e veloce; nell'implementazione hardware riesce ad essere anche più veloce.

\subsubsection{Modalità di cifratura}
I cifrari a blocchi sono "ripetitivi", nel senso che dato un testo in chiaro e una chiave, l'output sarà sempre uguale.
Questo può essere un problema per la sicurezza.

\paragraph{Modalità ECB}
Dato un testo in chiaro, con DES viene suddiviso in blocchi da 8 byte fino a raggiungere l'ultimo, che eventualmente viene riempito per raggiungere la grandezza richiesta, e vengono cifrati con la stessa chiave.
Questa tecnica è nota come ECB (Electronic Code Book).
Anche se i blocchi sono cifrati, è possibile manipolare il "libro" dei blocchi sapendo il contesto, senza il bisogno di decriptarli.

\paragraph{Modalità stream cipher}
Un blocco può venire danneggiato per un errore di trasmissione su un bit e in alcune applicazioni questo è inaccettabile.
La modalità stream cipher cerca di risolvere questo problema cifrando un vettore di inizializzazione per ottenere un blocco in uscita;
questo blocco viene cifrato per ottenere un secondo blocco in uscita, e così via.
La sequenza di blocchi cifrati è detta keystream e viene utilizzata come un blocco monouso e utilizzata in XOR con il testo in chiaro per ottenere il testo cifrato.

Il keystream dipende solo dalla chiave e dal vettore di inizializzazione, quindi non è sensibile agli errori di trasmissione del testo cifrato e permette la decifrazione generando lo stesso keystream. Un errore nel testo cifrato trasmesso genera un errore solo nel testo decifrato.

Non si deve mai usare la stessa coppia chiave-vettore, altrimenti si genera lo stesso keystream.
L'uso ripetuto dello stesso keystream espone il testo cifrato ad attacchi di keystream riutilizzato.

\subsection{Algoritmi a chiave pubblica} % 8.3 - TUTTO
Gli algoritmi a chiave pubblica sono stati proposti per risolvere un problema: le chiavi sono il punto primario, vanno distribuite ma senza lasciare che un intruso possa ottenerle.
Questi nuovi algoritmi propongono di usare chiavi di cifratura e decifratura diversi e non deducibili tra loro, e di rendere la chiave di cifratura pubblica insieme all'algoritmo.

\subsubsection{RSA}
RSA è un algoritmo a chiave pubblica ancora piuttosto solido, il cui principale svantaggio è l'uso di chiavi di almeno 1024 bit che lo rende un algoritmo lento.

L'algoritmo usa dei parametri pre calcolati.
Presi due numeri primi di 1024 bit, \textit{p} e \textit{q}, vengono calcolati i prodotti $n=p*q$ e $z=(p-1)*(q-1)$. 
Poi viene scelto un numero primo \textit{d} rispetto a \textit{z} e viene calcolato \textit{e} in modo che $e*d=1 \mod z$.
Il testo in chiaro viene poi diviso in blocchi di \textit{k} bit in modo che ogni messaggio sia compreso tra 0 e \textit{n} e $2^k < n$.
Per cifrare P viene calcolato $C=P^e(\mod n)$, mentre per decifrare $P=C^d (\mod n)$.
Quindi la chiave pubblica consiste nella coppia (e, n) mentre quella privata in (d, n).

Questo metodo è considerato sicuro per la difficoltà nel fattorizzare numeri molto grandi.
Tuttavia, data la lentezza dell'algoritmo, viene usato principalmente per distribuire le chiavi piuttosto che per grandi dati.

\subsection{Firme digitali} % 8.4
Nei documenti cartacei la firma ha un ruolo centrale. 
Anche in quelli digitali serve un modo per firmare i documenti e deve soddisfare alcune condizioni:
\begin{itemize}
\item il destinatario può dichiarare l'identità del mittente
\item il mittente non può ripudiare il contenuto del messaggio
\item il destinatario non può falsificare i messaggi del mittente
\end{itemize}
Un metodo è quello delle firme a chiave simmetrica, dove un'autorità affidabile e riconosciuta fa da intermediario.
Quando un utente A manda un messaggio a B, nel mezzo viene ricevuto, decifrato e inoltrato dall'autorità centrale che conferma l'identità del mittente.
Il problema è che l'entità di mezzo in questo modo può leggere i messaggi.

Un altro metodo è quello delle firme a chiave pubblica. 
Un utente A manda un messaggio P come $E_B(D_A(P))$ e l'utente B riesce ad ottenere il messaggio e a dimostrare che è un messaggio di A mostrando $D_A(P)$.
Questo funziona se la chiave segreta rimane tale e se non viene cambiata (che è lecito), altrimenti invalida la firma.

\subsubsection{Message digest} % 8.4.3 (partire da 8.4)
Un problema dei metodi di firma è il cercare di mettere assieme autenticazione e segretezza.
Esiste una tecnica che punta solo sull'autenticazione e usa una funzione di hash detta message digest. 
La funzione riceve il testo in chiaro e calcola una stringa di bit a lunghezza fissa;
deve essere facile calcolare MD(P) da P, ma impossibile calcolare P da MD(P).
Inoltre MD(P) dovrebbe essere univoco, quindi lungo almeno 128 bit, e se cambia 1 bit di input cambia totalmente il risultato.

Message digest è più rapido di cifrare l'intero testo con chiave pubblica e fa risparmiare nei costi di trasporto,
quindi è utile negli algoritmi di firma digitale.
Ad esempio prendendo il metodo delle firme a chiave simmetrica, l'intermediario al posto del messaggio decifrato calcola il message digest. 
Funziona anche con i sistemi a chiave pubblica.

Esistono diverse funzioni di message digest, tra cui MD5 e SHA-1.

\subsubsection{MD5}
Questa funzione mescola i bit d'ingresso in modo che ogni bit uscito sia influenzato da tutti i bit d'ingresso.
Il messaggio viene riempito fino a raggiungere la lunghezza di 448 bit (in modulo di 512) a cui vengono aggiunti 64 bit per indicare la lunghezza originale.
Vengono elaborati i blocchi di 512 bit:
ad ogni iterazione (x4) il blocco viene alterato usando un buffer di 128 bit inizializzato con un valore prefissato.
Alla fine dei blocchi, il contenuto del buffer è il message digest.

Al momento è un metodo ancora sicuro e funzionante.

\subsubsection{SHA-1}
Questa funzione elabora i blocchi da 512 bit ma produce una stringa più lunga di MD5, ovvero di 160 bit.

Anche qui il messaggio viene allungato finché non diventa multiplo di 512 e viene effettuato aggiungendo il bit 1 seguito da n bit 0.
La lunghezza originale viene indicata da un numero a 64 bit messo in OR con gli ultimi bit di ordine più basso.
L'algoritmo usa 5 variabili di 32 bit dove viene accumulato l'hash (con un procedimento che non verrà descritto qui).

Spesso si utilizza SHA-1 assieme a RSA in questo modo:
\begin{enumerate}
\item il testo in chiaro dell'utente A viene sia spedito all'utente B, sia processato da SHA-1
\item l'hash risultante da SHA-1 viene processato nell'algoritmo di RSA con la chiave privata dell'utente A
\item l'hash firmato risultante viene passato all'utente B
\item l'utente B calcola l'hash SHA-1 del testo in chiaro e usa la chiave pubblica di A sull'hash firmato: se combaciano i due hash il messaggio è valido
\end{enumerate}

\subsection{Sicurezza delle comunicazioni} % 8.6
\subsubsection{IPsec} % 8.6.1
Dopo una lunga disputa sul dove porre la sicurezza della rete, si è optato per lo strato network con IPsec (IP security).
Questo mette a disposizione diversi servizi, resi disponibili su richiesta (es. pagando): segretezza, integrità dei dati, protezione dagli attacchi ripetizione, tutti servizi basati sulla crittografia a chiave simmetrica per motivi di performance.
Inoltre permette diverse granularità (due host, due router, altro..) e rende disponibili diversi algoritmi, in quanto basarsi su un solo algoritmo può creare problemi in casi di vulnerabilità future.

IPsec è orientato alla connessione, in quanto servono le chiavi.
Nel contesto IPsec una connessione è detta Security Association.
SA è una connessione simplex; se serve un traffico sicuro in due direzioni, servono due SA.
Ogni SA ha un identificatore di sicurezza trasportato da pacchetti che viaggiano sulla connessione sicura.

Ci sono due modalità disponibili di IPsec:
\begin{itemize}
\item modalità trasporto, dove l'intestazione IPsec viene inserita dopo quella di IP.
\item modalità tunnel, dove il pacchetto IP viene incapsulato in un nuovo pacchetto IP con un'intestazione diversa; è utile quando il tunnel arriva in una destinazione diversa da quella finale, per esempio una macchina con gateway sicuro. Inoltre permette una difesa parziale dall'analisi del traffico (chi sta mandando pacchetti, dove e quanti). Il problema di questa modalità è che aumenta la dimensione del pacchetto.
\end{itemize}

L'intestazione della modalità trasporto è detta AH (Authentication Header), che garantisce il controllo dell'integrità, la sicurezza dagli attacchi di ripetizione, ma non la segretezza in quanto non permette la cifratura.
Nell'intestazione, il campo Next header field memorizza il valore originale del campo Protocol di IP, mentre il campo Payload length contiene il numero di word nell'intestazione -2.
Security parameters index identifica la connessione e sequence number indica il numero attribuito ai pacchetti inviati in una SA, diverso da quello originale (evita l'attacco di tipo ripetizione).
L'ultimo campo è Authentication data che contiene la firma digitale del payload (serve per controllare l'integrità).
L'algoritmo per la firma è deciso quando viene stabilita la SA.
Per motivi di performance, viene usata la crittografia a chiave simmetrica per il calcolo della firma;
la chiave è definita in fase di connessione.
Una tecnica per calcolare la firma è HMAC (Hashed Message Authentication Code), nella quale viene calcolato l'hash sui contenuti del pacchetto e della chiave.
\`E più veloce di SHA-1 + RSA.

Un'intestazione alternativa è ESP (Encapsulating Security Payload), con due word da 32 bit che costituiscono i campi security parameters index e sequence number, similmente a AH.
Può esserci una terza word con il vettore di inizializzazione se i dati sono cifrati.
Anche ESP permette di usare HMAC, ma non viene messo nell'intestazione, bensì in coda al campo payload, permettendo migliori prestazioni nelle implementazioni hardware.
ESP fa tutto quello che fa AH e di più, oltre che in maniera più efficiente.

\subsubsection{Firewall} % 8.6.2
Un'altra fonta di sicurezza per le reti è il firewall, usato per esempio nelle LAN aziendali.
Di fatto il traffico di rete viene fatto passare attraverso un unico "cancello" che controlla i dati prima di farli passare,
ad esempio due router con filtri dei pacchetti.
Questi router filtrano i pacchetti che non soddisfano certi criteri e vengono scartati, mentre gli altri passano.
Spesso i router con i filtri hanno tabelle con sorgenti e destinazioni accettate o rifiutate.
Per quando bloccare l'entrata sia più fattibile, magari bloccando determinate porte, controllare l'uscita è più complesso.
Inoltre le connessione UDP sono molto difficili da controllare, quindi spesso vengono bloccate in partenza.

La seconda parte del firewall è il gateway appicativo, il quale non guarda i pacchetti ma bensì funziona sulle applicazioni (es: mail).

Purtroppo il firewall non è perfetto. Un utente malevolo può inserire un indirizzo di sorgente falso per aggirare il firewall in entrata.
In uscita, spacciare informazioni è più facile grazie alle immagini che non essendo testuali, sono più difficili da controllare.

Infine c'è un problema non gestibile tramite firewall: gli attacchi DoS. 
In questi attacchi, l'utente manda una grande quantità di pacchetti legittimi per sovraccaricare il servizio,
per esempio, mandando molte richieste di connessione ma senza portarle avanti: in questo modo il servizio attaccato risponde e rimane in attesa della connessione, inutilmente.

Esiste una versione peggiore, il DDoS, ovvero quando l'intruso è già penetrato in diverse macchine e fa partire un attacco DoS simultaneo contro un servizio:
aumenta il potere di fuoco e diminuisce la possibilità di essere scoperto.

\subsubsection{Sicurezza wireless} % 8.6.4
Quando si introduce il wireless in una rete, la vulnerabilità aumenta.

\paragraph{802.11}
Lo standard 802.11 usa il protocollo per la sicurezza chiamato WEP (Wired Equivalent Privacy) per lo strato data link.
Ogni stazione stabilisce una chiave segreta condivisa con la stazione base.
WEP usa lo stream cipher basato sull'algoritmo RC4 che genera un keystream che applicato in XOR al testo in chiaro genera quello cifrato.
Il testo in chiaro è composto dal payload e il suo checksum calcolato con CRC-32 polinomiale. 
Il vettore di inizializzazione IV viene mandato con il testo cifrato, in modo che il ricevente possa usare la coppia chiave-IV per decifrare il testo.
Purtroppo non è un metodo sicuro in quanto presenta diverse vulnerabilità.

\paragraph{WAP 2.0}
Supporta l'uso di IPsec nello strato network, usa TLS (?) in quello di trasporto.

\subsection{Replay attack} % 8.7.3 
Il replay-attack è una forma di attacco di rete che consiste nell'impossessarsi di una credenziale di autenticazione comunicata da un host ad un altro, e riproporla successivamente simulando l'identità dell'emittente.
In genere l'azione viene compiuta da un attaccante che s'interpone tra i due lati comunicanti.

Questo attacco permette operazioni fraudolente come falsa autenticazione e/o transazioni duplicate, senza dover necessariamente decriptare la password, ma soltanto ritrasmettendola in un tempo successivo.

A differenza dell'attacco di tipo man in the middle che opera sempre in tempo reale, il replay attack può operare anche in modo asincrono quando la comunicazione originale è terminata.

Per esempio, si verifica un replay-attack quando Mallory intercetta la comunicazione tra Alice, che si sta autenticando con Bob, e si spaccia, agli occhi di Bob, per Alice.
Quando Bob chiede a Mallory (convinto di parlare con Alice) una chiave d'autenticazione, Mallory prontamente invia quella di Alice, instaurando così la comunicazione.

Esistono diverse contromisure.
Si possono usare token di sessione generati pseudocasualmente:
Bob invia ad Alice uno di questi token usa e getta che Alice utilizza per criptare la propria chiave da inviare a Bob (per esempio con una funzione di hashing che calcola il message digest della chiave concatenata con il token).
Bob effettua lo stesso calcolo e controlla che il suo risultato corrisponda con quello di Alice.
Mallory non può fare granché anche se ha catturato tale token di sessione, perché alla prossima comunicazione Alice e Bob si accorderanno con un altro token.
I token vanno memorizzati per sempre perché Mallory potrebbe attaccare anche anni dopo se non è stato memorizzato.

Un'altra contromisura è quella di utilizzare una marca temporale e di far sì che questa sia inserita nel corpo del messaggio criptato.

\subsection{Sicurezza del naming} % 8.9.2

\subsubsection{DNS spoofing}
Il DNS spoofing è una tecnica che consiste nell'installare un indirizzo IP falso in un server DNS.
Per fare ciò, l'utente malevolo manda una query ad server DNS per ottenere un certo indirizzo, ma si crea anche la risposta in modo che venga salvata nel server.
Questo è possibile perchè utilizzando UDP il server DNS non ha modo di sapere chi gli fornisce la risposta.
Il nuovo indirizzo viene salvato se non è presente nel server, altrimenti l'utente malevolo deve solo attendere che l'informazione scada, in modo da rimpiazzarla.
Una volta inserito il nuovo indirizzo, può ingannare l'utente bersaglio mostrando una pagina a sua scelta, magari una versione modificata della pagina obiettivo che permetta di carpire informazioni.

Perché lo spoofing funzioni, l'invasore deve conoscere e utilizzare un indirizzo IP di un server top level DNS, i quali però sono pubblici e facilmente falsificabili, e conoscere il numero di sequenza.
L'invasore può ottenere "facilmente" il numero se sono utilizzati sequenzialmente.

Esiste il progetto DNSsec che lavora sulla sicurezza del DNS, ma non è ancora diffuso.
Utilizza la crittografia a chiave pubblica. [...]
